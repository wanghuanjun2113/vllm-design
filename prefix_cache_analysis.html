<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>vLLM å®ç°åˆ†æ - Prefix Cache & Flash Attention</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.min.js"></script>
    <style>
        :root {
            --bg-primary: #0d1117;
            --bg-secondary: #161b22;
            --bg-tertiary: #21262d;
            --text-primary: #e6edf3;
            --text-secondary: #8b949e;
            --border-color: #30363d;
            --accent-blue: #58a6ff;
            --accent-green: #3fb950;
            --accent-orange: #d29922;
            --accent-red: #f85149;
            --accent-purple: #a371f7;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans', Helvetica, Arial, sans-serif;
            background: var(--bg-primary);
            color: var(--text-primary);
            line-height: 1.6;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
            padding: 20px;
        }

        header {
            text-align: center;
            padding: 40px 0;
            border-bottom: 1px solid var(--border-color);
            margin-bottom: 30px;
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
            background: linear-gradient(135deg, var(--accent-blue), var(--accent-purple));
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            background-clip: text;
        }

        .subtitle {
            color: var(--text-secondary);
            font-size: 1.1em;
        }

        .tabs {
            display: flex;
            gap: 5px;
            margin-bottom: 20px;
            flex-wrap: wrap;
            background: var(--bg-secondary);
            padding: 10px;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        .tab {
            padding: 12px 24px;
            background: transparent;
            border: none;
            color: var(--text-secondary);
            cursor: pointer;
            border-radius: 6px;
            font-size: 14px;
            font-weight: 500;
            transition: all 0.2s;
        }

        .tab:hover {
            background: var(--bg-tertiary);
            color: var(--text-primary);
        }

        .tab.active {
            background: var(--accent-blue);
            color: white;
        }

        .tab-content {
            display: none;
            background: var(--bg-secondary);
            padding: 30px;
            border-radius: 8px;
            border: 1px solid var(--border-color);
        }

        .tab-content.active {
            display: block;
        }

        h2 {
            color: var(--accent-blue);
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid var(--border-color);
        }

        h3 {
            color: var(--text-primary);
            margin: 25px 0 15px;
            font-size: 1.3em;
        }

        p {
            margin-bottom: 15px;
            color: var(--text-secondary);
        }

        .mermaid {
            background: var(--bg-primary);
            padding: 20px;
            border-radius: 8px;
            margin: 20px 0;
            border: 1px solid var(--border-color);
        }

        .diagram-container {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .info-box {
            background: var(--bg-tertiary);
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid var(--accent-blue);
            margin: 15px 0;
        }

        .info-box.success {
            border-left-color: var(--accent-green);
        }

        .info-box.warning {
            border-left-color: var(--accent-orange);
        }

        .info-box.error {
            border-left-color: var(--accent-red);
        }

        .data-structure {
            background: var(--bg-primary);
            padding: 20px;
            border-radius: 8px;
            border: 1px solid var(--border-color);
            margin: 15px 0;
        }

        .data-structure pre {
            background: var(--bg-tertiary);
            padding: 15px;
            border-radius: 6px;
            overflow-x: auto;
            color: var(--accent-green);
            font-size: 13px;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }

        th, td {
            padding: 12px 15px;
            text-align: left;
            border-bottom: 1px solid var(--border-color);
        }

        th {
            background: var(--bg-tertiary);
            color: var(--accent-blue);
            font-weight: 600;
        }

        tr:hover {
            background: var(--bg-tertiary);
        }

        .feature-list {
            list-style: none;
            padding: 0;
        }

        .feature-list li {
            padding: 10px 0 10px 30px;
            position: relative;
            color: var(--text-secondary);
        }

        .feature-list li:before {
            content: "âœ“";
            position: absolute;
            left: 0;
            color: var(--accent-green);
            font-weight: bold;
        }

        .badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: 500;
            margin-right: 8px;
        }

        .badge.blue { background: rgba(88, 166, 255, 0.2); color: var(--accent-blue); }
        .badge.green { background: rgba(63, 185, 80, 0.2); color: var(--accent-green); }
        .badge.orange { background: rgba(210, 153, 34, 0.2); color: var(--accent-orange); }
        .badge.purple { background: rgba(163, 113, 247, 0.2); color: var(--accent-purple); }

        .flow-step {
            display: flex;
            align-items: center;
            padding: 15px;
            margin: 10px 0;
            background: var(--bg-tertiary);
            border-radius: 8px;
            border-left: 4px solid var(--accent-blue);
        }

        .flow-step-number {
            width: 30px;
            height: 30px;
            background: var(--accent-blue);
            color: white;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            margin-right: 15px;
            font-weight: bold;
        }

        a {
            color: var(--accent-blue);
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .diagram-container {
                grid-template-columns: 1fr;
            }

            .tabs {
                flex-direction: column;
            }

            h1 {
                font-size: 1.8em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>vLLM æ ¸å¿ƒå®ç°åˆ†æ</h1>
            <p class="subtitle">Prefix Cache & Flash Attention æ·±åº¦è§£æ | vLLM Core Implementation Deep Dive</p>
        </header>

        <div class="tabs">
            <button class="tab active" onclick="showTab('overview')">æ¦‚è¿°</button>
            <button class="tab" onclick="showTab('architecture')">æ¶æ„è®¾è®¡</button>
            <button class="tab" onclick="showTab('request-flow')">è¯·æ±‚æµç¨‹</button>
            <button class="tab" onclick="showTab('cache-flow')">ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­</button>
            <button class="tab" onclick="showTab('data-structures')">æ•°æ®ç»“æ„</button>
            <button class="tab" onclick="showTab('hash')">å“ˆå¸Œè®¡ç®—</button>
            <button class="tab" onclick="showTab('eviction')">ç¼“å­˜æ·˜æ±°</button>
            <button class="tab" onclick="showTab('code')">ä»£ç æ˜ å°„</button>
            <button class="tab" onclick="showTab('fa-overview')">FA: æ¦‚è¿°</button>
            <button class="tab" onclick="showTab('fa-backend')">FA: åç«¯</button>
            <button class="tab" onclick="showTab('fa-modules')">FA: æ¨¡å—</button>
            <button class="tab" onclick="showTab('fa-flow')">FA: æµç¨‹</button>
            <button class="tab" onclick="showTab('fa-memory')">FA: ä¼˜åŒ–</button>
            <button class="tab" onclick="showTab('fa-mla')">FA: MLA</button>
            <button class="tab" onclick="showTab('fa-code')">FA: ä»£ç </button>
        </div>

        <!-- Overview Tab -->
        <div id="overview" class="tab-content active">
            <h2>ä»€ä¹ˆæ˜¯ Prefix Cache (å‰ç¼€ç¼“å­˜)</h2>

            <div class="info-box success">
                <h3>æ ¸å¿ƒæ¦‚å¿µ</h3>
                <p>Prefix Cache æ˜¯ vLLM ä¸­çš„ä¸€é¡¹å…³é”®ä¼˜åŒ–æŠ€æœ¯ï¼Œå®ƒç¼“å­˜å·²è®¡ç®—çš„ KV (Key-Value) cache blocksï¼Œä»¥ä¾¿åœ¨åç»­è¯·æ±‚ä¸­é‡ç”¨å…·æœ‰ç›¸åŒå‰ç¼€çš„åºåˆ—ã€‚</p>
            </div>

            <h3>ä¸»è¦ä¼˜åŠ¿</h3>
            <ul class="feature-list">
                <li>å‡å°‘è®¡ç®—æ—¶é—´ - é¿å…é‡æ–°è®¡ç®—ç›¸åŒçš„å‰ç¼€</li>
                <li>æé«˜ååé‡ - å¯¹äºæœ‰å…±äº«ä¸Šä¸‹æ–‡çš„åº”ç”¨ï¼ˆå¦‚èŠå¤©æœºå™¨äººã€ä»£ç è¡¥å…¨ï¼‰æ•ˆæœæ˜¾è‘—</li>
                <li>ä¿æŒç¼“å­˜ä¸€è‡´æ€§ - è·¨ä¸åŒè¯·æ±‚å…±äº«ç›¸åŒå†…å®¹çš„ç¼“å­˜</li>
                <li>æ”¯æŒå¤šç§å“ˆå¸Œç®—æ³• - æä¾›ä¸åŒçš„æ€§èƒ½/å®‰å…¨æƒè¡¡</li>
            </ul>

            <h3>å·¥ä½œåŸç†ç¤ºæ„</h3>
            <div class="mermaid">
sequenceDiagram
    participant U as ç”¨æˆ·è¯·æ±‚
    participant R as Requestå¯¹è±¡
    participant S as Scheduler
    participant C as Prefix Cache
    participant E as æ‰§è¡Œå¼•æ“

    U->>R: å‘é€è¯·æ±‚ "Hello, how are you?"
    R->>R: è®¡ç®—token hash
    R->>S: è°ƒåº¦è¯·æ±‚
    S->>C: æ£€æŸ¥å‰ç¼€ç¼“å­˜
    alt ç¼“å­˜å‘½ä¸­
        C-->>S: è¿”å›ç¼“å­˜çš„blocks
        Note over C,E: è·³è¿‡å·²ç¼“å­˜éƒ¨åˆ†çš„è®¡ç®—
    else ç¼“å­˜æœªå‘½ä¸­
        C-->>S: æ— ç¼“å­˜
        S->>E: æ‰§è¡Œå®Œæ•´è®¡ç®—
        E->>C: å­˜å‚¨æ–°è®¡ç®—çš„blocks
    end
    S->>E: ç»§ç»­æ‰§è¡Œå‰©ä½™tokens
    E-->>U: è¿”å›ç»“æœ
            </div>

            <h3>é…ç½®é€‰é¡¹</h3>
            <table>
                <tr>
                    <th>é…ç½®é¡¹</th>
                    <th>é»˜è®¤å€¼</th>
                    <th>è¯´æ˜</th>
                </tr>
                <tr>
                    <td><code>enable_prefix_caching</code></td>
                    <td><code>True</code></td>
                    <td>å¯ç”¨/ç¦ç”¨å‰ç¼€ç¼“å­˜</td>
                </tr>
                <tr>
                    <td><code>prefix_caching_hash_algo</code></td>
                    <td><code>"sha256"</code></td>
                    <td>å“ˆå¸Œç®—æ³•: sha256, sha256_cbor, xxhash, xxhash_cbor</td>
                </tr>
                <tr>
                    <td><code>block_size</code></td>
                    <td><code>16</code></td>
                    <td>æ¯ä¸ªblockåŒ…å«çš„tokenæ•°é‡</td>
                </tr>
            </table>

            <div class="diagram-container">
                <div class="info-box">
                    <h4>é€‚ç”¨åœºæ™¯</h4>
                    <p>âœ“ èŠå¤©æœºå™¨äºº - ç³»ç»Ÿæç¤ºè¯é‡å¤</p>
                    <p>âœ“ ä»£ç è¡¥å…¨ - ç›¸åŒçš„æ–‡ä»¶å¤´éƒ¨</p>
                    <p>âœ“ æ–‡æ¡£å¤„ç† - ç›¸åŒçš„æ–‡æ¡£æ¨¡æ¿</p>
                    <p>âœ“ æ‰¹é‡æ¨ç† - ç›¸åŒçš„è¾“å…¥å‰ç¼€</p>
                </div>
                <div class="info-box warning">
                    <h4>æ€§èƒ½è€ƒè™‘</h4>
                    <p>â€¢ ç¼“å­˜æŸ¥æ‰¾å¢åŠ å°‘é‡å»¶è¿Ÿ</p>
                    <p>â€¢ éœ€è¦é¢å¤–çš„å†…å­˜å­˜å‚¨hash map</p>
                    <p>â€¢ xxhash æ¯” sha256 æ›´å¿«ä½†å®‰å…¨æ€§è¾ƒä½</p>
                    <p>â€¢ æœ€ä½³ç¼“å­˜å‘½ä¸­ç‡å–å†³äºåº”ç”¨ç‰¹æ€§</p>
                </div>
            </div>
        </div>

        <!-- Architecture Tab -->
        <div id="architecture" class="tab-content">
            <h2>åˆ†å±‚æ¶æ„è®¾è®¡</h2>

            <p>vLLM çš„ Prefix Cache å®ç°é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œå„å±‚èŒè´£æ¸…æ™°ï¼š</p>

            <div class="mermaid">
graph TB
    R1["è¯·æ±‚å±‚ - Requestå¯¹è±¡"]
    S1["è°ƒåº¦å±‚ - Scheduler"]
    C1["åè°ƒå±‚ - KVCacheCoordinator"]
    M1["ç®¡ç†å±‚ - KVCacheManager"]
    B1["å­˜å‚¨å±‚ - BlockPool"]

    R1 --> S1
    S1 --> C1
    C1 --> M1
    M1 --> B1

    R2["è®¡ç®—Block Hashes"] --> R1
    S2["get_computed_blocks"] --> S1
    C2["åè°ƒcacheç»„"] --> C1
    M2["åˆ†é…slots"] --> M1
    B2["BlockHashToBlockMap"] --> B1

    style R1 fill:#58a6ff,color:#fff
    style S1 fill:#a371f7,color:#fff
    style C1 fill:#3fb950,color:#fff
    style M1 fill:#d29922,color:#fff
    style B1 fill:#f85149,color:#fff
            </div>

            <h3>å„å±‚èŒè´£è¯¦è§£</h3>

            <div class="flow-step">
                <div class="flow-step-number">1</div>
                <div>
                    <strong>è¯·æ±‚å±‚ (Request Layer)</strong>
                    <p><code>vllm/v1/core/request.py</code> - åˆ›å»ºRequestå¯¹è±¡æ—¶è®¡ç®—block hashes</p>
                </div>
            </div>

            <div class="flow-step">
                <div class="flow-step-number">2</div>
                <div>
                    <strong>è°ƒåº¦å±‚ (Scheduler Layer)</strong>
                    <p><code>vllm/v1/core/sched/scheduler.py</code> - é›†æˆç¼“å­˜æŸ¥æ‰¾åˆ°è°ƒåº¦å†³ç­–</p>
                </div>
            </div>

            <div class="flow-step">
                <div class="flow-step-number">3</div>
                <div>
                    <strong>åè°ƒå±‚ (Coordinator Layer)</strong>
                    <p><code>vllm/v1/core/kv_cache_coordinator.py</code> - åè°ƒä¸åŒcacheç±»å‹å’Œç»„</p>
                </div>
            </div>

            <div class="flow-step">
                <div class="flow-step-number">4</div>
                <div>
                    <strong>ç®¡ç†å±‚ (Manager Layer)</strong>
                    <p><code>vllm/v1/core/single_type_kv_cache_manager.py</code> - å¤„ç†åˆ†é…ã€ç¼“å­˜å’Œæ·˜æ±°</p>
                </div>
            </div>

            <div class="flow-step">
                <div class="flow-step-number">5</div>
                <div>
                    <strong>å­˜å‚¨å±‚ (Block Pool Layer)</strong>
                    <p><code>vllm/v1/core/block_pool.py</code> - ç®¡ç†å®é™…å­˜å‚¨å’Œæ£€ç´¢</p>
                </div>
            </div>

            <h3>æ ¸å¿ƒç»„ä»¶å…³ç³»å›¾</h3>
            <div class="mermaid">
graph LR
    Request[Request] --> Scheduler
    Scheduler --> Coordinator
    Coordinator --> Manager[KVCacheManager]
    Manager --> BlockPool
    BlockPool --> HashMap[BlockHashToBlockMap]
    BlockPool --> FreeQueue[FreeBlockQueue]
    BlockPool --> Blocks[KVCacheBlock]

    style Request fill:#58a6ff,color:#fff
    style HashMap fill:#3fb950,color:#fff
    style Blocks fill:#d29922,color:#fff
            </div>
        </div>

        <!-- Request Flow Tab -->
        <div id="request-flow" class="tab-content">
            <h2>è¯·æ±‚å¤„ç†å®Œæ•´æµç¨‹</h2>

            <p>ä»è¯·æ±‚åˆ°è¾¾vLLMåˆ°æ‰§è¡Œå®Œæˆçš„å®Œæ•´æµç¨‹ï¼Œå±•ç¤º Prefix Cache å¦‚ä½•èå…¥æ•´ä¸ªå¤„ç†è¿‡ç¨‹ï¼š</p>

            <div class="mermaid">
sequenceDiagram
    autonumber
    participant User as ç”¨æˆ·
    participant API as APIå±‚
    participant Req as Requeståˆ›å»º
    participant Hasher as BlockHasher
    participant Sched as Scheduler
    participant Coord as Coordinator
    participant Pool as BlockPool
    participant Worker as Workeræ‰§è¡Œ

    User->>API: å‘é€æ¨ç†è¯·æ±‚
    API->>Req: åˆ›å»ºRequestå¯¹è±¡
    Req->>Hasher: è·å–è¯·æ±‚block hasher
    Hasher-->>Req: è¿”å›BlockHasherå®ä¾‹
    Req->>Hasher: ä¸ºæ¯ä¸ªtokenè®¡ç®—hash
    Hasher-->>Req: è¿”å›block_hashesæ•°ç»„

    Req->>Sched: æäº¤è¯·æ±‚è°ƒåº¦
    Sched->>Coord: è°ƒç”¨get_computed_blocks()
    Coord->>Pool: æŸ¥æ‰¾æœ€é•¿çš„ç¼“å­˜å‘½ä¸­
    Pool->>Pool: åœ¨BlockHashToBlockMapä¸­æŸ¥æ‰¾

    alt æ‰¾åˆ°ç¼“å­˜å—
        Pool-->>Coord: è¿”å›ç¼“å­˜çš„blocks
        Coord-->>Sched: è¿”å›computed_blocks
        Sched->>Sched: è®¾ç½®num_cached_tokens
        Note over Sched: ä»ç¼“å­˜ç‚¹ç»§ç»­æ‰§è¡Œ
    else æœªæ‰¾åˆ°ç¼“å­˜
        Pool-->>Coord: è¿”å›ç©º
        Coord-->>Sched: è¿”å›ç©ºåˆ—è¡¨
        Note over Sched: ä»å¤´å¼€å§‹æ‰§è¡Œ
    end

    Sched->>Worker: æäº¤æ‰§è¡Œä»»åŠ¡
    Worker->>Pool: è·å–/åˆ†é…blocks
    Worker-->>User: è¿”å›ç”Ÿæˆç»“æœ
            </div>

            <h3>å…³é”®æ­¥éª¤è¯¦è§£</h3>

            <div class="diagram-container">
                <div class="info-box">
                    <h4>æ­¥éª¤ 1-4: è¯·æ±‚åˆå§‹åŒ–</h4>
                    <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/core/request.py</code></p>
                    <p>è¯·æ±‚åˆ›å»ºæ—¶ï¼Œé€šè¿‡ <code>get_request_block_hasher()</code> è·å–hashè®¡ç®—å™¨ï¼Œä¸ºæ¯ä¸ªtoken blockè®¡ç®—å“ˆå¸Œå€¼ã€‚</p>
                    <pre>block_hashes = hasher.compute_all_block_hashes(token_ids)</pre>
                </div>

                <div class="info-box">
                    <h4>æ­¥éª¤ 5-8: ç¼“å­˜æŸ¥æ‰¾</h4>
                    <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/core/sched/scheduler.py:495-527</code></p>
                    <p>è°ƒåº¦å™¨è°ƒç”¨ <code>get_computed_blocks()</code> æŸ¥æ‰¾å¯ç”¨çš„ç¼“å­˜å—ã€‚</p>
                    <pre>computed_blocks = coordinator.get_computed_blocks(
    self.block_ids, self.block_hashes
)</pre>
                </div>

                <div class="info-box success">
                    <h4>æ­¥éª¤ 9: ç¼“å­˜å‘½ä¸­å¤„ç†</h4>
                    <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/core/kv_cache_manager.py:164-204</code></p>
                    <p>å¦‚æœæ‰¾åˆ°ç¼“å­˜ï¼Œè®¾ç½® <code>num_cached_tokens</code>ï¼Œé¿å…é‡æ–°è®¡ç®—ã€‚</p>
                    <pre>self.num_cached_tokens = len(computed_blocks) * block_size</pre>
                </div>

                <div class="info-box warning">
                    <h4>æ­¥éª¤ 10: ç¼“å­˜æœªå‘½ä¸­å¤„ç†</h4>
                    <p>å¦‚æœæœªæ‰¾åˆ°ç¼“å­˜ï¼Œæ­£å¸¸åˆ†é…æ–°blockså¹¶æ‰§è¡Œè®¡ç®—ã€‚</p>
                </div>
            </div>

            <h3>Block Hash è®¡ç®—æµç¨‹</h3>
            <div class="mermaid">
flowchart TD
    Start[è¯·æ±‚åˆ°è¾¾] --> GetTokenizer[è·å–Tokenizer]
    GetTokenizer --> GetHasher[è·å–BlockHasher]
    GetHasher --> SetTokenIDs[è®¾ç½®token_ids]

    SetTokenIDs --> ComputeHash[è®¡ç®—ç¬¬ä¸€ä¸ªblock hash]
    ComputeHash --> AddSalt{æ˜¯å¦æœ‰Extra Keys}

    AddSalt --> AddExtra[æ·»åŠ MMç‰¹å¾LoRA ID]
    AddSalt --> UseTokens[ä»…ä½¿ç”¨tokens]

    AddExtra --> HashFunc[è°ƒç”¨hashå‡½æ•°]
    UseTokens --> HashFunc

    HashFunc --> SelectAlgo{é€‰æ‹©ç®—æ³•}
    SelectAlgo --> SHA256[SHA256 Pickle]
    SelectAlgo --> CBOR[SHA256 CBOR]
    SelectAlgo --> XXHASH[xxhash Pickle]
    SelectAlgo --> XXCBOR[xxhash CBOR]

    SHA256 --> StoreHash[å­˜å‚¨hash]
    CBOR --> StoreHash
    XXHASH --> StoreHash
    XXCBOR --> StoreHash

    StoreHash --> NextBlock{è¿˜æœ‰æ›´å¤štokens}
    NextBlock --> ComputeParent[ä½¿ç”¨parent hashè®¡ç®—]
    NextBlock --> End[å®Œæˆ]

    ComputeParent --> AddSalt

    style Start fill:#58a6ff,color:#fff
    style End fill:#3fb950,color:#fff
    style SelectAlgo fill:#a371f7,color:#fff
            </div>
        </div>

        <!-- Cache Flow Tab -->
        <div id="cache-flow" class="tab-content">
            <h2>ç¼“å­˜å‘½ä¸­/æœªå‘½ä¸­æµç¨‹</h2>

            <h3>ç¼“å­˜æŸ¥æ‰¾å†³ç­–æ ‘</h3>
            <div class="mermaid">
flowchart TD
    Start([è¯·æ±‚åˆ°è¾¾]) --> ComputeHash[è®¡ç®—Block Hashes]
    ComputeHash --> Schedule[è°ƒåº¦è¯·æ±‚]
    Schedule --> Lookup{åœ¨BlockHashToBlockMapä¸­æŸ¥æ‰¾}

    Lookup --> Hit[ç¼“å­˜å‘½ä¸­]
    Lookup --> Miss[ç¼“å­˜æœªå‘½ä¸­]

    Hit --> CheckRefCount{æ£€æŸ¥ref_cnt}
    CheckRefCount --> Increment[å¢åŠ å¼•ç”¨è®¡æ•°]
    CheckRefCount --> Reactivate[é‡æ–°æ¿€æ´»block]

    Increment --> UseCached[ä½¿ç”¨ç¼“å­˜çš„blocks]
    Reactivate --> UseCached

    UseCached --> SetCached[è®¾ç½®num_cached_tokens]
    SetCached --> Continue[ä»ç¼“å­˜ç‚¹ç»§ç»­æ‰§è¡Œ]

    Miss --> CheckFree{æœ‰free blocks}
    CheckFree --> AllocateNew[åˆ†é…æ–°blocks]
    CheckFree --> NeedEvict[éœ€è¦æ·˜æ±°ç¼“å­˜]

    AllocateNew --> ComputeBlocks[æ‰§è¡Œè®¡ç®—]
    NeedEvict --> Evict[æ‰§è¡ŒLRUæ·˜æ±°]
    Evict --> AllocateNew
    AllocateNew --> ComputeBlocks

    ComputeBlocks --> FullBlocks{blocksæ»¡äº†}
    FullBlocks --> ComputeHash2[è®¡ç®—block hash]
    FullBlocks --> WaitComplete[ç­‰å¾…å®Œæˆ]

    ComputeHash2 --> StoreCache[å­˜å…¥ç¼“å­˜]
    StoreCache --> WaitComplete
    WaitComplete --> Continue

    Continue --> Complete([è¯·æ±‚å®Œæˆ])

    style Start fill:#58a6ff,color:#fff
    style Hit fill:#3fb950,color:#fff
    style Miss fill:#f85149,color:#fff
    style Complete fill:#a371f7,color:#fff
            </div>

            <h3>ç¼“å­˜å‘½ä¸­è¯¦ç»†æµç¨‹</h3>
            <div class="diagram-container">
                <div class="info-box success">
                    <h4>å‘½ä¸­æ£€æµ‹</h4>
                    <p><strong>ä»£ç ä½ç½®:</strong> <code>block_pool.py:get_cached_block()</code></p>
                    <p>é€šè¿‡ <code>BlockHashWithGroupId</code> åœ¨ <code>cached_block_hash_to_block</code> å­—å…¸ä¸­æŸ¥æ‰¾ã€‚</p>
                    <pre>block = self.cached_block_hash_to_block.get_one_block(
    BlockHashWithGroupId(hash_bytes + group_id)
)</pre>
                </div>

                <div class="info-box">
                    <h4>å¼•ç”¨è®¡æ•°ç®¡ç†</h4>
                    <p><strong>ä»£ç ä½ç½®:</strong> <code>block_pool.py:acquire_block()</code></p>
                    <p>æ‰¾åˆ°ç¼“å­˜blockåï¼Œå¢åŠ å…¶å¼•ç”¨è®¡æ•°ï¼š</p>
                    <pre>block.ref_cnt += 1</pre>
                </div>

                <div class="info-box">
                    <h4>è®¾ç½®ç¼“å­˜tokenæ•°</h4>
                    <p><strong>ä»£ç ä½ç½®:</strong> <code>scheduler.py:509</code></p>
                    <pre>self.num_cached_tokens = len(computed_blocks) * self.block_size</pre>
                </div>
            </div>

            <h3>ç¼“å­˜æœªå‘½ä¸­è¯¦ç»†æµç¨‹</h3>
            <div class="diagram-container">
                <div class="info-box warning">
                    <h4>åˆ†é…æ–°blocks</h4>
                    <p><strong>ä»£ç ä½ç½®:</strong> <code>block_pool.py:allocate()</code></p>
                    <p>ä»free_block_queueè·å–å¯ç”¨blockï¼Œæˆ–è§¦å‘æ·˜æ±°ã€‚</p>
                </div>

                <div class="info-box warning">
                    <h4>æ‰§è¡Œæ¨¡å‹è®¡ç®—</h4>
                    <p>Workeræ‰§è¡Œæ¨¡å‹å‰å‘ä¼ æ’­ï¼Œè®¡ç®—æ–°çš„KV cacheã€‚</p>
                </div>

                <div class="info-box">
                    <h4>Blockå®Œæˆåç¼“å­˜</h4>
                    <p><strong>ä»£ç ä½ç½®:</strong> <code>block_pool.py:mark_block_as_cached()</code></p>
                    <p>å½“blockå¡«æ»¡ï¼ˆ16ä¸ªtokensï¼‰åï¼Œè®¡ç®—hashå¹¶å­˜å…¥ç¼“å­˜ï¼š</p>
                    <pre>block._block_hash = BlockHashWithGroupId(computed_hash)
self.cached_block_hash_to_block.insert(block._block_hash, block)</pre>
                </div>
            </div>

            <h3>ç¼“å­˜æŸ¥æ‰¾æ—¶åºå›¾</h3>
            <div class="mermaid">
sequenceDiagram
    participant S as Scheduler
    participant C as Coordinator
    participant M as KVCacheManager
    participant P as BlockPool
    participant H as HashMap

    S->>C: get_computed_blocks(block_ids, block_hashes)
    C->>M: éå†æ¯ä¸ªblock hash
    M->>P: get_cached_block(hash)

    P->>H: æŸ¥æ‰¾ hash -> block æ˜ å°„
    H-->>P: è¿”å›ç»“æœ

    alt æ‰¾åˆ°åŒ¹é…
        P-->>M: è¿”å› cached_block
        M->>P: acquire_block(block_id)
        P->>P: block.ref_cnt += 1
        P-->>M: ç¡®è®¤
        M-->>C: æ·»åŠ åˆ° computed_blocks
    else æœªæ‰¾åˆ°
        P-->>M: è¿”å› None
        C->>C: åœæ­¢æŸ¥æ‰¾ï¼ˆåç»­blockä¹Ÿæ— ç¼“å­˜ï¼‰
    end

    C-->>S: è¿”å› computed_blocks åˆ—è¡¨
            </div>
        </div>

        <!-- Data Structures Tab -->
        <div id="data-structures" class="tab-content">
            <h2>æ ¸å¿ƒæ•°æ®ç»“æ„</h2>

            <h3>KVCacheBlock - KVç¼“å­˜å—</h3>
            <div class="data-structure">
                <p>å­˜å‚¨å•ä¸ªKV cache blockçš„å…ƒæ•°æ®å’ŒçŠ¶æ€ã€‚</p>
                <pre>@dataclass
class KVCacheBlock:
    """KV-cache block metadata with prefix caching support"""

    # å”¯ä¸€æ ‡è¯†ç¬¦
    block_id: int

    # å¼•ç”¨è®¡æ•° - è·Ÿè¸ªæœ‰å¤šå°‘è¯·æ±‚åœ¨ä½¿ç”¨æ­¤block
    ref_cnt: int = 0

    # Blockçš„å“ˆå¸Œå€¼ - ç”¨äºprefix cacheæŸ¥æ‰¾
    # Noneè¡¨ç¤ºblockæœªè¢«ç¼“å­˜
    _block_hash: BlockHashWithGroupId | None = None

    # ç‰¹æ®Šæ ‡è®° - null blockæ°¸è¿œä¸åº”è¯¥è¢«ç¼“å­˜
    is_null: bool = False</pre>
            </div>

            <h3>BlockHashToBlockMap - å“ˆå¸Œåˆ°å—æ˜ å°„</h3>
            <div class="data-structure">
                <p>æ ¸å¿ƒçš„ç¼“å­˜æŸ¥æ‰¾ç»“æ„ï¼Œå°†block hashç›´æ¥æ˜ å°„åˆ°blockå¯¹è±¡ã€‚</p>
                <pre>class BlockHashToBlockMap:
    """Cache mapping block hash directly to block(s)"""

    # å†…éƒ¨å­—å…¸ç»“æ„:
    # - å¯¹äºå•ä¸ªblock: hash -> KVCacheBlock
    # - å¯¹äºå¤šä¸ªç›¸åŒhashçš„blocks: hash -> {block_id: KVCacheBlock}
    _cache: dict[BlockHashWithGroupId,
                 KVCacheBlock | dict[int, KVCacheBlock]]

    def get_one_block(self, key: BlockHashWithGroupId
                     ) -> KVCacheBlock | None:
        """è·å–å•ä¸ªåŒ¹é…çš„block"""

    def insert(self, key: BlockHashWithGroupId,
               block: KVCacheBlock) -> None:
        """æ’å…¥æ–°çš„ç¼“å­˜æ¡ç›®"""

    def pop(self, key: BlockHashWithGroupId,
            block_id: int) -> KVCacheBlock | None:
        """ç§»é™¤å¹¶è¿”å›æŒ‡å®šblock"""</pre>
            </div>

            <h3>BlockPool - Blockæ± ç®¡ç†å™¨</h3>
            <div class="data-structure">
                <p>ç®¡ç†æ‰€æœ‰KV cache blocksï¼ŒåŒ…æ‹¬ç¼“å­˜å’Œç©ºé—²blocksã€‚</p>
                <pre>class BlockPool:
    """Manages KVCacheBlocks with prefix caching support"""

    # ========== ç¼“å­˜ç›¸å…³ ==========
    # Hashåˆ°blockçš„æ˜ å°„è¡¨ - æ ¸å¿ƒç¼“å­˜ç»“æ„
    cached_block_hash_to_block: BlockHashToBlockMap

    # ========== Blockå­˜å‚¨ ==========
    # æ‰€æœ‰blocksçš„åˆ—è¡¨
    blocks: list[KVCacheBlock]

    # ========== ç©ºé—²Blockç®¡ç† ==========
    # ç©ºé—²blocké˜Ÿåˆ— - ç”¨äºå¿«é€Ÿåˆ†é…
    free_block_queue: FreeKVCacheBlockQueue

    # ========== é…ç½® ==========
    # æ˜¯å¦å¯ç”¨å‰ç¼€ç¼“å­˜
    enable_caching: bool

    # æ¯ä¸ªblockçš„tokenæ•°é‡
    block_size: int

    # å“ˆå¸Œç®—æ³•
    hash_fn: HashFunction</pre>
            </div>

            <h3>Request - è¯·æ±‚å¯¹è±¡</h3>
            <div class="data-structure">
                <p>è¯·æ±‚å¯¹è±¡æºå¸¦block hashesç”¨äºç¼“å­˜æŸ¥æ‰¾ã€‚</p>
                <pre>class Request:
    """Request with prefix caching support"""

    # ========== ç¼“å­˜ç›¸å…³ ==========
    # æ¯ä¸ªblockçš„hashå€¼
    block_hashes: list[BlockHashWithGroupId | None]

    # ä»ç¼“å­˜ä¸­è·å–çš„tokenæ•°é‡
    num_cached_tokens: int = 0

    # ========== æ‰§è¡Œç›¸å…³ ==========
    # è¯·æ±‚çš„token IDs
    token_ids: list[int]

    # åˆ†é…çš„block IDs
    block_ids: list[int]</pre>
            </div>

            <h3>æ•°æ®ç»“æ„å…³ç³»å›¾</h3>
            <div class="mermaid">
graph TB
    subgraph "BlockPool"
        BP[BlockPoolå®ä¾‹]
        HM[BlockHashToBlockMap]
        FQ[FreeBlockQueue]
        BL[KVCacheBlock[]]

        BP --> HM
        BP --> FQ
        BP --> BL
    end

    subgraph "å•ä¸ªKVCacheBlock"
        KB[KVCacheBlock]
        ID[block_id: int]
        RC[ref_cnt: int]
        BH[_block_hash: BlockHashWithGroupId]
        IN[is_null: bool]

        KB --> ID
        KB --> RC
        KB --> BH
        KB --> IN
    end

    subgraph "Request"
        RQ[Request]
        BH2[block_hashes: list]
        NCT[num_cached_tokens]
        BI[block_ids: list]

        RQ --> BH2
        RQ --> NCT
        RQ --> BI
    end

    subgraph "BlockHashToBlockMapå†…éƒ¨"
        HM2[_cache: dict]
        K1[BlockHashWithGroupId]
        V1[KVCacheBlock | dict]
        K2[BlockHashWithGroupId]
        V2[{block_id: KVCacheBlock}]

        HM2 --> K1
        HM2 --> V1
        HM2 --> K2
        HM2 --> V2
    end

    RQ --> BP
    HM --> KB

    style BP fill:#58a6ff,color:#fff
    style KB fill:#3fb950,color:#fff
    style RQ fill:#d29922,color:#fff
    style HM2 fill:#a371f7,color:#fff
            </div>
        </div>

        <!-- Hash Computation Tab -->
        <div id="hash" class="tab-content">
            <h2>å“ˆå¸Œè®¡ç®—æœºåˆ¶</h2>

            <p>Block hashæ˜¯Prefix Cacheçš„æ ¸å¿ƒï¼Œå®ƒåŸºäºblockçš„å†…å®¹è®¡ç®—å”¯ä¸€æ ‡è¯†ç¬¦ã€‚</p>

            <h3>å“ˆå¸Œç±»å‹å®šä¹‰</h3>
            <div class="data-structure">
                <pre># å•ä¸ªblockçš„å“ˆå¸Œå€¼
BlockHash = NewType("BlockHash", bytes)

# å¸¦group IDçš„blockå“ˆå¸Œï¼ˆç”¨äºå¤šcacheç»„åœºæ™¯ï¼‰
BlockHashWithGroupId = NewType("BlockHashWithGroupId", bytes)

# ç»„æˆ: hash_bytes(32å­—èŠ‚) + group_id(4å­—èŠ‚) = 36å­—èŠ‚</pre>
            </div>

            <h3>å“ˆå¸Œè®¡ç®—æµç¨‹</h3>
            <div class="mermaid">
flowchart TD
    Start([è®¡ç®—Block Hash]) --> Input[è¾“å…¥å‚æ•°]

    Input --> P1[parent_block_hash: bytes]
    Input --> P2[token_ids: list[int]]
    Input --> P3[extra_keys: dict]

    P1 --> Serialize[åºåˆ—åŒ–æ•°æ®]
    P2 --> Serialize
    P3 --> Serialize

    Serialize --> AddSalt[æ·»åŠ cache_salt]
    AddSalt --> Select{é€‰æ‹©ç®—æ³•}

    Select --> S1[SHA256_Hash]
    Select --> S2[SHA256_CBOR]
    Select --> S3[xxHash_Hash]
    Select --> S4[xxHash_CBOR]

    S1 --> Append[é™„åŠ group_id]
    S2 --> Append
    S3 --> Append
    S4 --> Append

    Append --> Output[BlockHashWithGroupId]
    Output --> End([è¿”å›36å­—èŠ‚hash])

    style Start fill:#58a6ff,color:#fff
    style Select fill:#a371f7,color:#fff
    style End fill:#3fb950,color:#fff
            </div>

            <h3>æ”¯æŒçš„å“ˆå¸Œç®—æ³•</h3>
            <table>
                <tr>
                    <th>ç®—æ³•</th>
                    <th>åºåˆ—åŒ–</th>
                    <th>é€Ÿåº¦</th>
                    <th>å®‰å…¨æ€§</th>
                    <th>è·¨è¯­è¨€</th>
                    <th>æ¨èåœºæ™¯</th>
                </tr>
                <tr>
                    <td><span class="badge blue">sha256</span></td>
                    <td>Pickle</td>
                    <td>ä¸­ç­‰</td>
                    <td>é«˜</td>
                    <td>å¦</td>
                    <td>é»˜è®¤ï¼ŒPythonç¯å¢ƒ</td>
                </tr>
                <tr>
                    <td><span class="badge purple">sha256_cbor</span></td>
                    <td>CBOR</td>
                    <td>ä¸­ç­‰</td>
                    <td>é«˜</td>
                    <td>æ˜¯</td>
                    <td>è·¨è¯­è¨€éƒ¨ç½²</td>
                </tr>
                <tr>
                    <td><span class="badge green">xxhash</span></td>
                    <td>Pickle</td>
                    <td>å¿«</td>
                    <td>éåŠ å¯†</td>
                    <td>å¦</td>
                    <td>é«˜æ€§èƒ½éœ€æ±‚</td>
                </tr>
                <tr>
                    <td><span class="badge orange">xxhash_cbor</span></td>
                    <td>CBOR</td>
                    <td>æœ€å¿«</td>
                    <td>éåŠ å¯†</td>
                    <td>æ˜¯</td>
                    <td>æœ€é«˜æ€§èƒ½ + è·¨è¯­è¨€</td>
                </tr>
            </table>

            <h3>Hash Chain (å“ˆå¸Œé“¾)</h3>
            <p>ä¸ºäº†éªŒè¯è¿ç»­æ€§ï¼Œæ¯ä¸ªblockçš„hashè®¡ç®—ä¾èµ–äºå‰ä¸€ä¸ªblockçš„hashï¼š</p>

            <div class="mermaid">
graph LR
    T1[Tokens 0-15] --> H1[Hash_1]
    T2[Tokens 16-31] --> H2["Hash_2 parent=Hash_1"]
    T3[Tokens 32-47] --> H3["Hash_3 parent=Hash_2"]
    T4[Tokens 48-63] --> H4["Hash_4 parent=Hash_3"]

    H1 --> H2
    H2 --> H3
    H3 --> H4

    style H1 fill:#58a6ff,color:#fff
    style H2 fill:#3fb950,color:#fff
    style H3 fill:#d29922,color:#fff
    style H4 fill:#a371f7,color:#fff
            </div>

            <div class="info-box">
                <h4>Hash Chain çš„ä½œç”¨</h4>
                <ul>
                    <li><strong>éªŒè¯è¿ç»­æ€§:</strong> ç¡®ä¿blocksæ˜¯æŒ‰é¡ºåºè®¡ç®—çš„</li>
                    <li><strong>å‰ç¼€åŒ¹é…:</strong> å¯ä»¥å¿«é€Ÿæ‰¾åˆ°æœ€é•¿å…¬å…±å‰ç¼€</li>
                    <li><strong>é˜²ç¯¡æ”¹:</strong> ä»»ä½•blockçš„ä¿®æ”¹éƒ½ä¼šå½±å“åç»­æ‰€æœ‰hashes</li>
                </ul>
            </div>

            <h3>Extra Keys (é¢å¤–é”®)</h3>
            <p>é™¤äº†tokensï¼Œhashè®¡ç®—è¿˜åŒ…å«ä»¥ä¸‹é¢å¤–ä¿¡æ¯ä»¥ç¡®ä¿æ­£ç¡®æ€§ï¼š</p>

            <div class="diagram-container">
                <div class="info-box">
                    <h4>Multi-Modal Features</h4>
                    <p>å›¾åƒã€éŸ³é¢‘ç­‰å¤šæ¨¡æ€ç‰¹å¾çš„å“ˆå¸Œå€¼</p>
                    <pre>extra_keys['mm_features'] = hash(mm_input)</pre>
                </div>

                <div class="info-box">
                    <h4>LoRA Adapter ID</h4>
                    <p>ä½¿ç”¨çš„LoRAé€‚é…å™¨ID</p>
                    <pre>extra_keys['lora_id'] = lora_request_id</pre>
                </div>

                <div class="info-box">
                    <h4>Cache Salt</h4>
                    <p>ç”¨äºåŒºåˆ†ç›¸åŒå†…å®¹çš„ä¸åŒè¯·æ±‚</p>
                    <pre>extra_keys['salt'] = cache_salt_value</pre>
                </div>
            </div>

            <h3>ä»£ç å®ç°ä½ç½®</h3>
            <table>
                <tr>
                    <th>å‡½æ•°/æ–¹æ³•</th>
                    <th>æ–‡ä»¶</th>
                    <th>è¡Œå·</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><code>hash_block_tokens()</code></td>
                    <td><code>vllm/v1/core/kv_cache_utils.py</code></td>
                    <td>525-606</td>
                    <td>æ ¸å¿ƒhashè®¡ç®—å‡½æ•°</td>
                </tr>
                <tr>
                    <td><code>get_request_block_hasher()</code></td>
                    <td><code>vllm/v1/core/kv_cache_utils.py</code></td>
                    <td>608-650</td>
                    <td>è·å–è¯·æ±‚çš„hashè®¡ç®—å™¨</td>
                </tr>
                <tr>
                    <td><code>BlockHasher.__call__()</code></td>
                    <td><code>vllm/v1/core/kv_cache_utils.py</code></td>
                    <td>~520</td>
                    <td>è®¡ç®—å•ä¸ªblock hash</td>
                </tr>
            </table>
        </div>

        <!-- Eviction Tab -->
        <div id="eviction" class="tab-content">
            <h2>ç¼“å­˜æ·˜æ±°ç­–ç•¥</h2>

            <p>vLLMä½¿ç”¨LRUï¼ˆLeast Recently Usedï¼‰ç­–ç•¥æ·˜æ±°ç¼“å­˜ï¼Œå½“éœ€è¦æ–°çš„blocksä½†æ²¡æœ‰ç©ºé—²blocksæ—¶è§¦å‘ã€‚</p>

            <h3>æ·˜æ±°æµç¨‹å›¾</h3>
            <div class="mermaid">
flowchart TD
    Start([éœ€è¦åˆ†é…æ–°Block]) --> CheckFree{FreeBlockQueueä¸ºç©º}

    CheckFree --> UseFree[ä»FreeQueueåˆ†é…]
    CheckFree --> CheckCached{æœ‰å¯æ·˜æ±°çš„cached blocks}

    CheckCached --> Error[æ— å¯ç”¨å†…å­˜]
    CheckCached --> SelectLRU[é€‰æ‹©LRU cached block]

    SelectLRU --> CheckRef{ref_cntä¸º0}
    CheckRef --> FindNext[æ‰¾ä¸‹ä¸€ä¸ªLRU]
    CheckRef --> Evict[æ‰§è¡Œæ·˜æ±°]

    FindNext --> CheckRef

    Evict --> RemoveHash[ä»HashMapç§»é™¤]
    RemoveHash --> ClearHash[æ¸…é™¤block hash]
    ClearHash --> AddToFree[åŠ å…¥FreeQueue]
    AddToFree --> UseFree

    UseFree --> Increment[å¢åŠ ref_cnt]
    Increment --> ReturnBlock[è¿”å›Block]
    ReturnBlock --> End([å®Œæˆ])

    Error --> End

    style Start fill:#58a6ff,color:#fff
    style Error fill:#f85149,color:#fff
    style End fill:#3fb950,color:#fff
    style Evict fill:#d29922,color:#fff
            </div>

            <h3>æ·˜æ±°å…³é”®æ–¹æ³•</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/core/block_pool.py:331-369</code></p>
                <pre>def _maybe_evict_cached_block(self) -> KVCacheBlock | None:
    """LRUæ·˜æ±°ç­–ç•¥å®ç°"""

    # éå†cached blocksï¼ˆæŒ‰LRUé¡ºåºï¼‰
    for block in self.cached_blocks:
        # åªèƒ½æ·˜æ±°å¼•ç”¨è®¡æ•°ä¸º0çš„block
        if block.ref_cnt == 0:
            # 1. ä»hash mapä¸­ç§»é™¤
            self.cached_block_hash_to_block.pop(
                block._block_hash, block.block_id
            )

            # 2. æ¸…é™¤blockçš„hash
            block._block_hash = None

            # 3. è¿”å›è¯¥blockä¾›é‡ç”¨
            return block

    # æ²¡æœ‰å¯æ·˜æ±°çš„block
    return None</pre>
            </div>

            <h3>LRUç»´æŠ¤æœºåˆ¶</h3>
            <div class="diagram-container">
                <div class="info-box">
                    <h4>è®¿é—®æ›´æ–°</h4>
                    <p>æ¯æ¬¡è®¿é—®ç¼“å­˜blockæ—¶ï¼Œæ›´æ–°å…¶LRUä½ç½®ï¼š</p>
                    <pre># å½“blockè¢«acquireæ—¶
self.cached_blocks.move_to_end(block)</pre>
                </div>

                <div class="info-box">
                    <h4>æ’å…¥æ–°ç¼“å­˜</h4>
                    <p>æ–°ç¼“å­˜çš„blockæ·»åŠ åˆ°LRUé“¾è¡¨æœ«å°¾ï¼š</p>
                    <pre>self.cached_blocks.append(block)</pre>
                </div>

                <div class="info-box warning">
                    <h4>å¼•ç”¨è®¡æ•°æ£€æŸ¥</h4>
                    <p>åªèƒ½æ·˜æ±° <code>ref_cnt == 0</code> çš„blocksï¼Œç¡®ä¿ä¸ä¼šæ·˜æ±°æ­£åœ¨ä½¿ç”¨çš„blockã€‚</p>
                </div>
            </div>

            <h3>æ·˜æ±°æ—¶åºå›¾</h3>
            <div class="mermaid">
sequenceDiagram
    participant Alloc as Allocator
    participant Pool as BlockPool
    participant Queue as FreeQueue
    participant Cached as CachedBlocks
    participant Map as HashMap

    Alloc->>Queue: å°è¯•è·å–free block
    Queue-->>Alloc: ç©ºï¼ˆæ— å¯ç”¨blockï¼‰

    Alloc->>Pool: éœ€è¦æ·˜æ±°ç¼“å­˜
    Pool->>Cached: è·å–LRU block

    Pool->>Pool: æ£€æŸ¥ref_cnt == 0?

    alt å¯ä»¥æ·˜æ±°
        Pool->>Map: ç§»é™¤hashæ˜ å°„
        Map-->>Pool: ç¡®è®¤ç§»é™¤

        Pool->>Pool: æ¸…é™¤block._block_hash
        Pool->>Queue: åŠ å…¥free queue

        Queue-->>Alloc: è¿”å›å¯ç”¨block
    else ä¸èƒ½æ·˜æ±°
        Pool-->>Alloc: è¿”å›Noneï¼ˆå†…å­˜ä¸è¶³ï¼‰
    end

    style Cached fill:#f85149,color:#fff
    style Map fill:#a371f7,color:#fff
            </div>

            <h3>æ·˜æ±°ç­–ç•¥ç‰¹ç‚¹</h3>

            <div class="diagram-container">
                <div class="info-box success">
                    <h4>âœ“ å®‰å…¨æ€§</h4>
                    <p>åªæ·˜æ±°å¼•ç”¨è®¡æ•°ä¸º0çš„blocksï¼Œä¸ä¼šå½±å“æ­£åœ¨æ‰§è¡Œçš„è¯·æ±‚</p>
                </div>

                <div class="info-box success">
                    <h4>âœ“ æ•ˆç‡</h4>
                    <p>LRUç­–ç•¥ä¿ç•™çƒ­é—¨çš„å‰ç¼€ï¼Œæé«˜ç¼“å­˜å‘½ä¸­ç‡</p>
                </div>

                <div class="info-box success">
                    <h4>âœ“ ç®€å•</h4>
                    <p>å®ç°ç®€å•ï¼Œç»´æŠ¤æˆæœ¬ä½</p>
                </div>

                <div class="info-box warning">
                    <h4>âš  å±€é™æ€§</h4>
                    <p>ä¸åŒºåˆ†blockçš„é‡è¦æ€§ï¼Œå¯èƒ½æ·˜æ±°å¤§çš„å‰ç¼€è€Œä¿ç•™å°çš„</p>
                </div>
            </div>
        </div>

        <!-- Code Mapping Tab -->
        <div id="code" class="tab-content">
            <h2>ä»£ç æ–‡ä»¶æ˜ å°„</h2>

            <p>Prefix Cache å®ç°æ¶‰åŠå¤šä¸ªæ–‡ä»¶ï¼Œä»¥ä¸‹æ˜¯å…³é”®æ–‡ä»¶åŠå…¶åŠŸèƒ½ï¼š</p>

            <table>
                <thead>
                    <tr>
                        <th>ç»„ä»¶</th>
                        <th>æ–‡ä»¶è·¯å¾„</th>
                        <th>å…³é”®ç±»/å‡½æ•°</th>
                        <th>åŠŸèƒ½</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><span class="badge blue">Block Pool</span></td>
                        <td><code>vllm/v1/core/block_pool.py</code></td>
                        <td>
                            <code>BlockPool</code><br>
                            <code>BlockHashToBlockMap</code><br>
                            <code>FreeKVCacheBlockQueue</code>
                        </td>
                        <td>ç®¡ç†KV cache blocksï¼Œç¼“å­˜æŸ¥æ‰¾å’Œæ·˜æ±°</td>
                    </tr>
                    <tr>
                        <td><span class="badge purple">Cache Manager</span></td>
                        <td><code>vllm/v1/core/kv_cache_manager.py</code></td>
                        <td>
                            <code>SingleTypeKVCacheManager</code><br>
                            <code>get_computed_blocks()</code><br>
                            <code>allocate_slots()</code>
                        </td>
                        <td>ç¼“å­˜ç®¡ç†æ¥å£ï¼Œå¤„ç†å‘½ä¸­/æœªå‘½ä¸­</td>
                    </tr>
                    <tr>
                        <td><span class="badge green">Hash Utils</span></td>
                        <td><code>vllm/v1/core/kv_cache_utils.py</code></td>
                        <td>
                            <code>hash_block_tokens()</code><br>
                            <code>get_request_block_hasher()</code><br>
                            <code>BlockHasher</code>
                        </td>
                        <td>å“ˆå¸Œè®¡ç®—ç›¸å…³å·¥å…·å‡½æ•°</td>
                    </tr>
                    <tr>
                        <td><span class="badge orange">Scheduler</span></td>
                        <td><code>vllm/v1/core/sched/scheduler.py</code></td>
                        <td>
                            <code>Scheduler.schedule()</code><br>
                            <code>get_computed_blocks()</code>
                        </td>
                        <td>é›†æˆç¼“å­˜æŸ¥æ‰¾åˆ°è°ƒåº¦é€»è¾‘</td>
                    </tr>
                    <tr>
                        <td><span class="badge blue">Request</span></td>
                        <td><code>vllm/v1/core/request.py</code></td>
                        <td>
                            <code>Request.__init__()</code><br>
                            <code>block_hashes</code>
                        </td>
                        <td>è¯·æ±‚å¯¹è±¡ï¼Œæºå¸¦block hashes</td>
                    </tr>
                    <tr>
                        <td><span class="badge purple">Coordinator</span></td>
                        <td><code>vllm/v1/core/kv_cache_coordinator.py</code></td>
                        <td>
                            <code>KVCacheCoordinator</code><br>
                            <code>find_longest_cache_hit()</code>
                        </td>
                        <td>åè°ƒå¤šä¸ªcacheç»„</td>
                    </tr>
                    <tr>
                        <td><span class="badge green">Config</span></td>
                        <td><code>vllm/config/cache.py</code></td>
                        <td>
                            <code>CacheConfig</code><br>
                            <code>enable_prefix_caching</code><br>
                            <code>prefix_caching_hash_algo</code>
                        </td>
                        <td>ç¼“å­˜é…ç½®</td>
                    </tr>
                </tbody>
            </table>

            <h3>å…³é”®ä»£ç ç‰‡æ®µ</h3>

            <div class="info-box">
                <h4>1. ç¼“å­˜æŸ¥æ‰¾ (block_pool.py)</h4>
                <pre>def get_cached_block(self,
                      block_hash: BlockHashWithGroupId
                     ) -> KVCacheBlock | None:
    """é€šè¿‡hashæŸ¥æ‰¾ç¼“å­˜çš„block"""
    return self.cached_block_hash_to_block.get_one_block(block_hash)</pre>
            </div>

            <div class="info-box">
                <h4>2. å‘½ä¸­æ£€æµ‹ (scheduler.py:495-527)</h4>
                <pre># æŸ¥æ‰¾å¯ç”¨çš„ç¼“å­˜blocks
computed_blocks = self.coordinator.get_computed_blocks(
    self.block_ids, self.block_hashes
)

if computed_blocks:
    # å‘½ä¸­ï¼šè®¾ç½®ç¼“å­˜tokenæ•°
    self.num_cached_tokens = len(computed_blocks) * block_size</pre>
            </div>

            <div class="info-box">
                <h4>3. å“ˆå¸Œè®¡ç®— (kv_cache_utils.py:525-606)</h4>
                <pre>def hash_block_tokens(
    hash_fn: HashFunction,
    parent_block_hash: bytes,
    token_ids: list[int],
    extra_keys: dict | None,
) -> bytes:
    """è®¡ç®—blockçš„hashå€¼"""
    # åºåˆ—åŒ–æ•°æ®
    data = {
        'parent': parent_block_hash,
        'tokens': token_ids,
        'extra': extra_keys or {}
    }
    # åº”ç”¨hashå‡½æ•°
    return hash_fn(data)</pre>
            </div>

            <div class="info-box">
                <h4>4. Blockç¼“å­˜ (block_pool.py)</h4>
                <pre>def mark_block_as_cached(self,
                         block: KVCacheBlock,
                         block_hash: BlockHashWithGroupId):
    """æ ‡è®°blockä¸ºå·²ç¼“å­˜"""
    block._block_hash = block_hash
    self.cached_block_hash_to_block.insert(block_hash, block)
    self.cached_blocks.append(block)  # æ·»åŠ åˆ°LRUæœ«å°¾</pre>
            </div>

            <h3>æ–‡ä»¶ä¾èµ–å…³ç³»</h3>
            <div class="mermaid">
graph TD
    Config[config/cache.py] --> Manager[v1/core/kv_cache_manager.py]
    Utils[v1/core/kv_cache_utils.py] --> Request[v1/core/request.py]
    Utils --> Manager

    Request --> Scheduler[v1/core/sched/scheduler.py]
    Manager --> Scheduler
    Manager --> Coordinator[v1/core/kv_cache_coordinator.py]
    Manager --> Pool[v1/core/block_pool.py]

    Coordinator --> Pool

    style Config fill:#d29922
    style Pool fill:#58a6ff,color:#fff
    style Manager fill:#a371f7,color:#fff
    style Scheduler fill:#3fb950,color:#fff
            </div>

            <div class="info-box">
                <h4>å¿«é€Ÿå¯¼èˆª</h4>
                <p>ğŸ“ <strong>æ ¸å¿ƒå®ç°:</strong> <code>vllm/v1/core/block_pool.py</code></p>
                <p>ğŸ“ <strong>Hashè®¡ç®—:</strong> <code>vllm/v1/core/kv_cache_utils.py</code></p>
                <p>ğŸ“ <strong>è°ƒåº¦é›†æˆ:</strong> <code>vllm/v1/core/sched/scheduler.py</code></p>
                <p>ğŸ“ <strong>é…ç½®é€‰é¡¹:</strong> <code>vllm/config/cache.py</code></p>
            </div>
        </div>

        <!-- Flash Attention Overview Tab -->
        <div id="fa-overview" class="tab-content">
            <h2>Flash Attention æ¦‚è¿°</h2>

            <div class="info-box success">
                <h3>æ ¸å¿ƒæ¦‚å¿µ</h3>
                <p>Flash Attention æ˜¯ä¸€ç§ I/O æ„ŸçŸ¥çš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œé€šè¿‡å¹³é“ºå’Œé‡æ–°è®¡ç®—æ¥æœ€å°åŒ–å†…å­˜è®¿é—®ï¼Œåœ¨ä¸ç‰ºç‰²ç²¾åº¦çš„å‰æä¸‹å¤§å¹…æå‡æ³¨æ„åŠ›è®¡ç®—é€Ÿåº¦ã€‚</p>
            </div>

            <h3>ä¸»è¦ä¼˜åŠ¿</h3>
            <ul class="feature-list">
                <li>å†…å­˜é«˜æ•ˆ - é¿å…ç‰©åŒ–å·¨å¤§çš„æ³¨æ„åŠ›çŸ©é˜µ</li>
                <li>è®¡ç®—ä¼˜åŒ– - ä½¿ç”¨å¹³é“ºå’Œå—çº§è®¡ç®—æœ€å¤§åŒ– GPU åˆ©ç”¨ç‡</li>
                <li>Paged KV é›†æˆ - ä¸ vLLM çš„åˆ†é¡µå†…å­˜ç®¡ç†æ— ç¼é›†æˆ</li>
                <li>å¤šåç«¯æ”¯æŒ - FA2ã€FA3ã€FlashInferã€Triton ç­‰å¤šç§å®ç°</li>
                <li>FP8 é‡åŒ– - æ”¯æŒä½ç²¾åº¦è®¡ç®—ä»¥è¿›ä¸€æ­¥å‡å°‘å†…å­˜</li>
            </ul>

            <h3>æ”¯æŒçš„æ³¨æ„åŠ›åç«¯å¯¹æ¯”</h3>
            <table>
                <tr>
                    <th>åç«¯åç§°</th>
                    <th>ä¾èµ–åº“</th>
                    <th>æ€§èƒ½</th>
                    <th>ç‰¹æ€§</th>
                    <th>é€‚ç”¨åœºæ™¯</th>
                </tr>
                <tr>
                    <td><span class="badge blue">FLASH_ATTN</span></td>
                    <td>flash-attn</td>
                    <td>é«˜</td>
                    <td>FA2/FA3 æ”¯æŒã€å¹¿æ³›ä½¿ç”¨</td>
                    <td>é€šç”¨åœºæ™¯</td>
                </tr>
                <tr>
                    <td><span class="badge purple">FLASHINFER</span></td>
                    <td>flashinfer</td>
                    <td>æé«˜</td>
                    <td>TRTLLM é›†æˆã€åŸç”Ÿ paged KV</td>
                    <td>é«˜æ€§èƒ½éœ€æ±‚</td>
                </tr>
                <tr>
                    <td><span class="badge green">TRITON_ATTN</span></td>
                    <td>triton</td>
                    <td>ä¸­</td>
                    <td>çº¯ Pythonã€æ— å¤–éƒ¨ä¾èµ–</td>
                    <td>å…¼å®¹æ€§ä¼˜å…ˆ</td>
                </tr>
                <tr>
                    <td><span class="badge orange">FLASHMLA</span></td>
                    <td>flash-attn</td>
                    <td>é«˜</td>
                    <td>å‹ç¼© KV cache (MLA)</td>
                    <td>DeepSeek-V3 ç­‰</td>
                </tr>
            </table>

            <h3>Flash Attention å·¥ä½œæµç¨‹</h3>
            <div class="mermaid">
sequenceDiagram
    participant Input as è¾“å…¥tokens
    participant QKV as QKVæŠ•å½±
    participant Cache as KV Cache
    participant FA as Flash Attention
    participant Output as è¾“å‡º

    Input->>QKV: æŠ•å½±åˆ°Q,K,V
    QKV->>QKV: Q: [batch, seq, heads, head_dim]
    QKV->>QKV: K,V: [batch, seq, kv_heads, head_dim]

    alt æœ‰æ–°çš„KV pairs
        QKV->>Cache: reshape_and_cache_flash()
        Cache->>Cache: å­˜å‚¨åˆ°paged KV cache
    end

    QKV->>FA: è°ƒç”¨flash_attn_varlen_func
    FA->>Cache: ä»paged cacheè¯»å–KV
    Cache-->>FA: è¿”å›KVæ•°æ®

    FA->>FA: å—çº§è®¡ç®—attention
    FA->>FA: ä½¿ç”¨SRAMå¹³é“ºå‡å°‘HBMè®¿é—®

    FA->>FA: Softmax + MatMul
    FA-->>Output: Attentionè¾“å‡º
            </div>

            <div class="diagram-container">
                <div class="info-box">
                    <h4>å†…å­˜ä¼˜åŒ–æŠ€æœ¯</h4>
                    <p><strong>Tiling (å¹³é“º):</strong> å°†æ³¨æ„åŠ›è®¡ç®—åˆ†è§£ä¸ºé€‚åˆ SRAM çš„å—</p>
                    <p><strong>Recompute (é‡è®¡ç®—):</strong> åå‘ä¼ æ’­æ—¶é‡è®¡ç®—è€Œéå­˜å‚¨æ³¨æ„åŠ›çŸ©é˜µ</p>
                </div>
                <div class="info-box warning">
                    <h4>vLLM ç‰¹æœ‰ä¼˜åŒ–</h4>
                    <p><strong>Paged KV Cache:</strong> éè¿ç»­å†…å­˜å¸ƒå±€</p>
                    <p><strong>Block Tables:</strong> åŠ¨æ€æ˜ å°„ token åˆ°ç‰©ç†å—</p>
                    <p><strong>FP8 é‡åŒ–:</strong> KV cache ä½¿ç”¨ 8bit æµ®ç‚¹</p>
                </div>
            </div>

            <h3>Prefill vs Decode</h3>
            <div class="mermaid">
flowchart TD
    Start[Attentionè¯·æ±‚] --> CheckType{è¯·æ±‚ç±»å‹}

    CheckType --> Prefill[Prefillè·¯å¾„]
    CheckType --> Decode[Decodeè·¯å¾„]

    Prefill --> PrefillKernel[FlashAttn Varlen]
    Decode --> DecodeKernel[Single Token Kernel]

    PrefillKernel --> BatchOpt[æ‰¹é‡çŸ©é˜µä¹˜æ³•ä¼˜åŒ–]
    DecodeKernel --> CacheOpt[Paged Cacheä¼˜åŒ–]

    BatchOpt --> Output1[Attentionè¾“å‡º]
    CacheOpt --> Output2[Attentionè¾“å‡º]

    style Prefill fill:#58a6ff,color:#fff
    style Decode fill:#3fb950,color:#fff
            </div>
        </div>

        <!-- Flash Attention Backend Architecture -->
        <div id="fa-backend" class="tab-content">
            <h2>åç«¯æ¶æ„è®¾è®¡</h2>

            <h3>åç«¯ç±»å±‚æ¬¡ç»“æ„</h3>
            <div class="mermaid">
graph TD
    Backend[AttentionBackendæŠ½è±¡åŸºç±»]
    FA_Backend[FlashAttentionBackend]
    FI_Backend[FlashInferBackend]
    TR_Backend[TritonAttentionBackend]
    MLA_Backend[FlashMLABackend]

    Backend --> FA_Backend
    Backend --> FI_Backend
    Backend --> TR_Backend
    Backend --> MLA_Backend

    Backend --> B1[str]
    Backend --> B2[type]
    Backend --> B3[type]

    FA_Backend --> FA1[FLASH_ATTN]
    FI_Backend --> FI1[FLASHINFER]
    TR_Backend --> TR1[TRITON_ATTN]
    MLA_Backend --> MLA1[FLASHMLA]

    style Backend fill:#a371f7,color:#fff
    style FA_Backend fill:#58a6ff
    style FI_Backend fill:#3fb950
    style TR_Backend fill:#d29922
    style MLA_Backend fill:#f85149
            </div>

            <h3>åç«¯é€‰æ‹©æµç¨‹</h3>
            <div class="mermaid">
flowchart TD
    Start[æ¨¡å‹åŠ è½½] --> GetConfig[è·å–AttentionConfig]
    GetConfig --> CheckHW{æ£€æµ‹GPUèƒ½åŠ›}

    CheckHW --> CheckVer{FAç‰ˆæœ¬}
    CheckHW --> TryTriton[å°è¯•Triton]

    CheckVer --> Validate3[éªŒè¯é…ç½®]
    CheckVer --> Validate2[éªŒè¯é…ç½®]

    Validate3 --> SelectFA3[é€‰æ‹©FLASH_ATTN_v3]
    Validate3 --> Validate2

    Validate2 --> SelectFA2[é€‰æ‹©FLASH_ATTN_v2]
    Validate2 --> TryTriton

    TryTriton --> ValidateT[éªŒè¯é…ç½®]
    ValidateT --> SelectTriton[é€‰æ‹©TRITON_ATTN]
    ValidateT --> Error[æ— å¯ç”¨åç«¯]

    SelectFA3 --> Init[åˆå§‹åŒ–åç«¯]
    SelectFA2 --> Init
    SelectTriton --> Init

    Init --> End[å°±ç»ª]

    style SelectFA3 fill:#58a6ff,color:#fff
    style SelectFA2 fill:#3fb950,color:#fff
    style SelectTriton fill:#d29922,color:#fff
            </div>

            <h3>åç«¯æ³¨å†Œç³»ç»Ÿ</h3>
            <div class="info-box">
                <h4>æ³¨å†Œæœºåˆ¶</h4>
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/registry.py</code></p>
                <p>åç«¯é€šè¿‡è£…é¥°å™¨è‡ªåŠ¨æ³¨å†Œåˆ°å…¨å±€æ³¨å†Œè¡¨ï¼š</p>
                <pre>@register_attention_backend("FLASH_ATTN")
class FlashAttentionBackend(AttentionBackend):
    ...</pre>
            </div>

            <h3>å…³é”®é…ç½®é€‰é¡¹</h3>
            <table>
                <tr>
                    <th>é…ç½®é¡¹</th>
                    <th>ç±»å‹</th>
                    <th>é»˜è®¤å€¼</th>
                    <th>è¯´æ˜</th>
                </tr>
                <tr>
                    <td><code>backend</code></td>
                    <td>str</td>
                    <td><code>"FLASH_ATTN"</code></td>
                    <td>æŒ‡å®šæ³¨æ„åŠ›åç«¯</td>
                </tr>
                <tr>
                    <td><code>flash_attn_version</code></td>
                    <td>int</td>
                    <td><code>2</code></td>
                    <td>Flash Attention ç‰ˆæœ¬ (2 æˆ– 3)</td>
                </tr>
                <tr>
                    <td><code>kv_cache_dtype</code></td>
                    <td>str</td>
                    <td><code>"auto"</code></td>
                    <td>KV cache æ•°æ®ç±»å‹ (fp8, auto)</td>
                </tr>
                <tr>
                    <td><code>use_cuda_graph</code></td>
                    <td>bool</td>
                    <td><code>True</code></td>
                    <td>æ˜¯å¦ä½¿ç”¨ CUDA Graph ä¼˜åŒ–</td>
                </tr>
            </table>
        </div>

        <!-- Flash Attention Module Details -->
        <div id="fa-modules" class="tab-content">
            <h2>æ¨¡å—/ç±»è¯¦è§£</h2>

            <h3>1. AttentionBackend (åŸºç±»)</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backend.py</code></p>
                <p><strong>ç›®çš„:</strong> å®šä¹‰æ‰€æœ‰æ³¨æ„åŠ›åç«¯çš„æŠ½è±¡æ¥å£</p>

                <h4>å…³é”®æ–¹æ³•:</h4>
                <table>
                    <tr><th>æ–¹æ³•</th><th>å‚æ•°</th><th>è¿”å›å€¼</th><th>åŠŸèƒ½</th></tr>
                    <tr><td><code>get_name()</code></td><td>-</td><td><code>str</code></td><td>è¿”å›åç«¯åç§°</td></tr>
                    <tr><td><code>get_impl_cls()</code></td><td>-</td><td><code>type</code></td><td>è¿”å›å®ç°ç±»</td></tr>
                    <tr><td><code>get_builder_cls()</code></td><td>-</td><td><code>type</code></td><td>è¿”å›å…ƒæ•°æ®æ„å»ºå™¨ç±»</td></tr>
                    <tr><td><code>supports_head_size()</code></td><td><code>head_size: int</code></td><td><code>bool</code></td><td>æ£€æŸ¥æ˜¯å¦æ”¯æŒè¯¥ head size</td></tr>
                    <tr><td><code>validate_configuration()</code></td><td>å¤šä¸ªé…ç½®å‚æ•°</td><td><code>str | None</code></td><td>éªŒè¯é…ç½®ï¼Œè¿”å›é”™è¯¯åŸå› æˆ–None</td></tr>
                </table>
            </div>

            <h3>2. FlashAttentionBackend</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flash_attn.py</code></p>
                <p><strong>ç›®çš„:</strong> Flash Attention åç«¯çš„ä¸»å…¥å£</p>

                <h4>å…³é”®å±æ€§:</h4>
                <pre>class FlashAttentionBackend(AttentionBackend):
    # æ”¯æŒçš„æ•°æ®ç±»å‹
    SUPPORTED_DTYPES = {torch.float16, torch.bfloat16}

    # æ”¯æŒçš„å—å¤§å° (å¿…é¡»æ˜¯16çš„å€æ•°)
    SUPPORTED_BLOCK_SIZES = {16, 32, 64}

    # æ”¯æŒçš„ FA ç‰ˆæœ¬
    SUPPORTED_FA_VERSIONS = {2, 3}</pre>

                <h4>é›†æˆç‚¹:</h4>
                <ul>
                    <li>ä½¿ç”¨: <code>Attention</code> å±‚</li>
                    <li>ä¾èµ–: <code>flash_attn</code> åº“</li>
                    <li>è°ƒç”¨: <code>flash_attn_varlen_func</code></li>
                </ul>
            </div>

            <h3>3. FlashAttentionImpl</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flash_attn.py</code></p>
                <p><strong>ç›®çš„:</strong> æ ¸å¿ƒ Flash Attention è®¡ç®—é€»è¾‘</p>

                <h4>å…³é”®å±æ€§:</h4>
                <pre>class FlashAttentionImpl(AttentionImpl):
    num_heads: int              # æ³¨æ„åŠ›å¤´æ•°
    head_size: int              # æ¯ä¸ªå¤´çš„ç»´åº¦
    scale: float                # attention scaling factor
    num_kv_heads: int           # KVå¤´æ•° (GQA/MQA)
    sliding_window: int | None  # æ»‘åŠ¨çª—å£å¤§å°
    kv_cache_dtype: str         # KV cache æ•°æ®ç±»å‹</pre>

                <h4>æ ¸å¿ƒæ–¹æ³• - forward()</h4>
                <pre>def forward(
    self,
    layer: AttentionLayerBase,
    query: torch.Tensor,
    key: torch.Tensor | None,
    value: torch.Tensor | None,
    kv_cache: torch.Tensor,
    attn_metadata: AttentionMetadata,
    output: torch.Tensor | None = None,
) -> torch.Tensor:
    """æ‰§è¡Œ Flash Attention å‰å‘ä¼ æ’­"""</pre>
            </div>

            <h3>4. FlashInferBackend</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flashinfer.py</code></p>
                <p><strong>ç›®çš„:</strong> FlashInfer é«˜æ€§èƒ½åç«¯</p>

                <h4>ç‰¹æ€§:</h4>
                <ul class="feature-list">
                    <li>TRTLLM é›†æˆ - é’ˆå¯¹ Blackwell GPU ä¼˜åŒ–</li>
                    <li>FP8 åŸç”Ÿæ”¯æŒ - ç¡¬ä»¶åŠ é€Ÿé‡åŒ–</li>
                    <li>Paged KV åŸç”Ÿæ”¯æŒ - æ— éœ€é¢å¤–è½¬æ¢</li>
                    <li>Prefill/Decode åˆ†ç¦» - ä¸“ç”¨å†…æ ¸ä¼˜åŒ–</li>
                </ul>

                <h4>KV Cache å½¢çŠ¶:</h4>
                <pre># FlashInfer ä½¿ç”¨ä¸åŒçš„å†…å­˜å¸ƒå±€
# Flash Attention: (2, num_blocks, block_size, num_kv_heads, head_size)
# FlashInfer: (num_blocks, 2, block_size, num_kv_heads, head_size)</pre>
            </div>

            <h3>5. FlashAttentionMetadataBuilder</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flash_attn.py</code></p>
                <p><strong>ç›®çš„:</strong> æ„å»º Flash Attention æ‰€éœ€çš„å…ƒæ•°æ®</p>

                <h4>å…³é”®ç»„ä»¶:</h4>
                <pre>class FlashAttentionMetadataBuilder(AttentionMetadataBuilder):
    scheduler_metadata: SchedulerMetadata | None  # AOT è°ƒåº¦ä¿¡æ¯
    aot_schedule: bool                             # æ˜¯å¦ä½¿ç”¨ AOT
    max_num_splits: int                            # CUDA graph æœ€å¤§åˆ†å‰²æ•°</pre>

                <h4>build() æ–¹æ³•:</h4>
                <pre>def build(self, common_data: CommonAttentionData
             ) -> FlashAttentionMetadata:
    """ä»å…¬å…±æ•°æ®æ„å»º Flash Attention å…ƒæ•°æ®"""
    return FlashAttentionMetadata(
        num_prefills=...,
        num_prefill_tokens=...,
        num_decode_tokens=...,
        slot_mapping=...,
        block_tables=...,
        seq_lens=...,
        ...
    )</pre>
            </div>

            <h3>6. FlashInferImpl</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flashinfer.py</code></p>
                <p><strong>ç›®çš„:</strong> FlashInfer å®ç°ç±»</p>

                <h4>å·¥ä½œç©ºé—´ç®¡ç†:</h4>
                <pre>class FlashInferImpl(AttentionImpl):
    # FlashInfer éœ€è¦å·¥ä½œç©ºé—´ç¼“å†²åŒº
    workspace_buffer: torch.Tensor | None = None

    def allocate_workspace(self, device: torch.device):
        """åˆ†é… FlashInfer å·¥ä½œç©ºé—´"""
        self.workspace_buffer = torch.empty(
            size, dtype=torch.int8, device=device
        )</pre>
            </div>

            <h3>7. FlashInferMetadataBuilder</h3>
            <div class="data-structure">
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flashinfer.py</code></p>
                <p><strong>ç›®çš„:</strong> FlashInfer å…ƒæ•°æ®æ„å»º</p>

                <h4>åˆ†ç¦»çš„å…ƒæ•°æ®è·¯å¾„:</h4>
                <pre># FlashInfer ä¸º prefill å’Œ decode ä½¿ç”¨ä¸åŒçš„å…ƒæ•°æ®
prefill_metadata = self.prefill_builder.build(common_data)
decode_metadata = self.decode_builder.build(common_data)</pre>
            </div>

            <h3>ç±»å…³ç³»å›¾</h3>
            <div class="mermaid">
graph TD
    Backend["AttentionBackendæŠ½è±¡åŸºç±»"]

    FA_Backend[FlashAttentionBackend] --> Backend
    FI_Backend[FlashInferBackend] --> Backend
    TR_Backend[TritonAttentionBackend] --> Backend

    FA_Impl[FlashAttentionImpl] --> FA_Backend
    FI_Impl[FlashInferImpl] --> FI_Backend

    FA_Builder[FlashAttentionMetadataBuilder] --> FA_Backend
    FI_Builder[FlashInferMetadataBuilder] --> FI_Backend

    FA_Impl --> FA_Builder
    FI_Impl --> FI_Builder

    style Backend fill:#a371f7,color:#fff
    style FA_Backend fill:#58a6ff
    style FI_Backend fill:#3fb950
    style FA_Impl fill:#d29922
    style FI_Impl fill:#f85149
            </div>
        </div>

        <!-- Flash Attention Computation Flow -->
        <div id="fa-flow" class="tab-content">
            <h2>Attention è®¡ç®—æµç¨‹</h2>

            <h3>å®Œæ•´çš„ Forward Pass åºåˆ—</h3>
            <div class="mermaid">
sequenceDiagram
    participant L as Attention Layer
    participant B as Backend
    participant K as KV Cache
    participant M as Metadata Builder
    participant G as GPU Kernel

    L->>M: build(common_data)
    M-->>L: FlashAttentionMetadata

    L->>B: forward(query, key, value, kv_cache, metadata)

    alt Encoder Attention
        B->>G: flash_attn_varlen_func (no cache)
        Note over G: ç¼–ç å™¨æ³¨æ„åŠ›æ— éœ€KV cache
    else Decoder Attention (æ ‡å‡†)
        B->>K: unbind(0) åˆ†ç¦» key_cache, value_cache

        alt æœ‰æ–°çš„ KV pairs
            B->>K: reshape_and_cache_flash()
            Note over K: å­˜å‚¨æ–°KVåˆ°paged cache
            K-->>B: å·²ç¼“å­˜
        end

        B->>G: flash_attn_varlen_func (with cache)
        G->>K: è¯»å– paged KV cache
        K-->>G: KV æ•°æ®

        G->>G: å—çº§è®¡ç®— attention scores
        G->>G: Softmax
        G->>G: çŸ©é˜µä¹˜æ³• (Q @ V)
        G-->>B: Attention è¾“å‡º
    end

    B-->>L: è¿”å› output
            </div>

            <h3>KV Cache é›†æˆæµç¨‹</h3>
            <div class="mermaid">
flowchart LR
    Input[Input Tokens] --> QKV[QKV Projection]
    QKV --> Q[Query Tensor]
    QKV --> K[Key Tensor]
    QKV --> V[Value Tensor]

    K --> CheckNew{æœ‰æ–°KV}
    V --> CheckNew

    CheckNew --> GetSlots[è·å–slot_mapping]
    CheckNew --> UseCached[ä½¿ç”¨ç¼“å­˜KV]

    GetSlots --> ReshapeCache[reshape_and_cache_flash]
    ReshapeCache --> PagedKV[Paged KV Cache]

    PagedKV --> FP8Check{FP8é‡åŒ–}
    FP8Check --> ConvertFP8[è½¬æ¢ä¸ºFP8]
    FP8Check --> KeepDtype[ä¿æŒåŸç±»å‹]

    ConvertFP8 --> AttnInput[Attentionè¾“å…¥]
    KeepDtype --> AttnInput
    UseCached --> AttnInput

    AttnInput --> FlashAttn[Flash Attention Kernel]
    FlashAttn --> Output[Attention Output]

    style PagedKV fill:#58a6ff,color:#fff
    style FlashAttn fill:#3fb950,color:#fff
            </div>

            <h3>Prefill vs Decode è¯¦ç»†å¯¹æ¯”</h3>
            <table>
                <tr>
                    <th>ç‰¹æ€§</th>
                    <th>Prefill é˜¶æ®µ</th>
                    <th>Decode é˜¶æ®µ</th>
                </tr>
                <tr>
                    <td>è¾“å…¥åºåˆ—é•¿åº¦</td>
                    <td>é•¿ (å¤štoken)</td>
                    <td>çŸ­ (1 token)</td>
                </tr>
                <tr>
                    <td>ä½¿ç”¨çš„å†…æ ¸</td>
                    <td><code>flash_attn_varlen_func</code></td>
                    <td><code>single_token_kernel</code></td>
                </tr>
                <tr>
                    <td>å†…å­˜è®¿é—®æ¨¡å¼</td>
                    <td>é¡ºåºè®¿é—®, å¯æ‰¹é‡</td>
                    <td>éšæœºè®¿é—® paged cache</td>
                </tr>
                <tr>
                    <td>ä¼˜åŒ–ç­–ç•¥</td>
                    <td>CUDA Graph, çº§è”æ³¨æ„</td>
                    <td>Paged KV, æ³¨æ„åŠ› sink</td>
                </tr>
                <tr>
                    <td>å…¸å‹ç”¨ä¾‹</td>
                    <td>æç¤ºè¯å¤„ç†, æ–‡æ¡£è¡¥å…¨</td>
                    <td>æ–‡æœ¬ç”Ÿæˆ, æµå¼è¾“å‡º</td>
                </tr>
            </table>

            <h3>Prefill/Decode è·¯å¾„å›¾</h3>
            <div class="mermaid">
flowchart TD
    Start[Attention Request] --> CheckType{Request Type}

    CheckType --> PrefillPath[Prefill Pathway]
    CheckType --> DecodePath[Decode Pathway]

    PrefillPath --> PrefillKernels[Prefill Kernels]
    DecodePath --> DecodeKernels[Decode Kernels]

    PrefillKernels --> PrefillOpts{ä¼˜åŒ–é€‰é¡¹}
    DecodeKernels --> DecodeOpts{ä¼˜åŒ–é€‰é¡¹}

    PrefillOpts --> PO1[FlashAttn Varlen]
    PrefillOpts --> PO2[Chunked Prefill]
    PrefillOpts --> PO3[Cascade Attention]

    DecodeOpts --> DO1[Paged Attention]
    DecodeOpts --> DO2[Attention Sinks]
    DecodeOpts --> DO3[Speculative Decode]

    PO1 --> Output[Attention Output]
    PO2 --> Output
    PO3 --> Output
    DO1 --> Output
    DO2 --> Output
    DO3 --> Output

    style PrefillPath fill:#58a6ff,color:#fff
    style DecodePath fill:#3fb950,color:#fff
            </div>

            <h3>å…³é”®ä»£ç ä½ç½®</h3>
            <div class="info-box">
                <h4>Forward Pass å…¥å£</h4>
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/backends/flash_attn.py</code></p>
                <p><strong>æ–¹æ³•:</strong> <code>FlashAttentionImpl.forward()</code></p>
                <p><strong>è¡Œå·:</strong> ~200-300</p>
            </div>

            <div class="info-box">
                <h4>KV Cache æ“ä½œ</h4>
                <p><strong>æ–‡ä»¶:</strong> <code>vllm/v1/attention/ops/paged_attn.py</code></p>
                <p><strong>å‡½æ•°:</strong> <code>reshape_and_cache_flash()</code></p>
            </div>
        </div>

        <!-- Flash Attention Memory Optimization -->
        <div id="fa-memory" class="tab-content">
            <h2>å†…å­˜ä¼˜åŒ–æŠ€æœ¯</h2>

            <h3>1. Tiling (å¹³é“ºæŠ€æœ¯)</h3>
            <div class="info-box">
                <h4>åŸç†</h4>
                <p>å°†å¤§çš„æ³¨æ„åŠ›çŸ©é˜µåˆ†è§£ä¸ºé€‚åˆ GPU SRAM çš„å°å—ï¼Œå‡å°‘ HBM (é«˜å¸¦å®½å†…å­˜) è®¿é—®æ¬¡æ•°ã€‚</p>
            </div>

            <div class="mermaid">
flowchart LR
    Input["Q K V Tensors"] --> Split[åˆ†å‰²æˆå—]

    Split --> Tile1["Tile 1 - Q1 K1 V1"]
    Split --> Tile2["Tile 2 - Q2 K2 V2"]
    Split --> TileN["Tile N - QN KN VN"]

    Tile1 --> Compute1[åœ¨SRAMä¸­è®¡ç®—]
    Tile2 --> Compute2[åœ¨SRAMä¸­è®¡ç®—]
    TileN --> ComputeN[åœ¨SRAMä¸­è®¡ç®—]

    Compute1 --> O1[O1]
    Compute2 --> O2[O2]
    ComputeN --> ON[ON]

    O1 --> Concat[è¿æ¥è¾“å‡º]
    O2 --> Concat
    ON --> Concat

    Concat --> Output[æœ€ç»ˆè¾“å‡º]

    style Compute1 fill:#3fb950,color:#fff
    style Compute2 fill:#3fb950,color:#fff
    style ComputeN fill:#3fb950,color:#fff
            </div>

            <h3>2. FP8 é‡åŒ–</h3>
            <div class="data-structure">
                <p><strong>æ”¯æŒçš„ FP8 æ ¼å¼:</strong></p>
                <pre># FP8 E4M3 (è®­ç»ƒ/æ¨ç†)
torch.float8_e4m3fn  # æ­£æ•°èŒƒå›´å¤§ï¼Œç²¾åº¦ç•¥ä½

# FP8 E5M2 (æƒé‡å­˜å‚¨)
torch.float8_e5m2  # èŒƒå›´å°ï¼Œç²¾åº¦é«˜</pre>

                <h4>é‡åŒ–æµç¨‹:</h4>
                <pre># KV Cache ä½¿ç”¨ FP8 å­˜å‚¨
kv_cache_dtype = "fp8_e4m3"

# reshape_and_cache æ—¶è½¬æ¢
key_cache = key.to(torch.float8_e4m3fn)
value_cache = value.to(torch.float8_e4m3fn)</pre>
            </div>

            <h3>3. Paged Memory Layout</h3>
            <div class="mermaid">
graph TB
    subgraph "é€»è¾‘ KV Cache"
        L1[Token 0-15]
        L2[Token 16-31]
        L3[Token 32-47]
        L4[Token 48-63]
    end

    subgraph "ç‰©ç† Paged Cache"
        P1["Block 0 - Token 0-15"]
        P2["Block 5 - Token 16-31"]
        P3["Block 2 - Token 32-47"]
        P4["Block 9 - Token 48-63"]
    end

    L1 --> P1
    L2 --> P2
    L3 --> P3
    L4 --> P4

    BT["Block Tableç´¢å¼•æ˜ å°„"] --> P1
    BT --> P2
    BT --> P3
    BT --> P4

    style P1 fill:#58a6ff
    style P2 fill:#3fb950
    style P3 fill:#d29922
    style P4 fill:#a371f7
            </div>

            <h3>4. CUDA Graph ä¼˜åŒ–</h3>
            <div class="info-box">
                <h4>åŸç†</h4>
                <p>é¢„å…ˆç¼–è¯‘ kernel å›¾ï¼Œå‡å°‘ kernel å¯åŠ¨å¼€é”€ã€‚é€‚ç”¨äºå½¢çŠ¶ä¸€è‡´çš„é‡å¤è®¡ç®—ã€‚</p>

                <h4>vLLM å®ç°:</h4>
                <pre># scheduler_metadata ç”¨äº CUDA Graph
scheduler_metadata = SchedulerMetadata(
    num_prefills=num_prefills,
    num_prefill_tokens=num_prefill_tokens,
    max_num_splits=max_num_splits,
)</pre>
            </div>

            <h3>å†…å­˜ä¼˜åŒ–å¯¹æ¯”</h3>
            <table>
                <tr>
                    <th>ä¼˜åŒ–æŠ€æœ¯</th>
                    <th>å†…å­˜èŠ‚çœ</th>
                    <th>æ€§èƒ½å½±å“</th>
                    <th>å®ç°å¤æ‚åº¦</th>
                </tr>
                <tr>
                    <td>Tiling</td>
                    <td>~10x (ä¸­é—´ç»“æœ)</td>
                    <td>æ˜¾è‘—æå‡</td>
                    <td>é«˜</td>
                </tr>
                <tr>
                    <td>FP8 é‡åŒ–</td>
                    <td>2x (KV cache)</td>
                    <td>è½»å¾®ä¸‹é™</td>
                    <td>ä½</td>
                </tr>
                <tr>
                    <td>Paged Memory</td>
                    <td>å‡å°‘ç¢ç‰‡</td>
                    <td>æ— å½±å“</td>
                    <td>ä¸­</td>
                </tr>
                <tr>
                    <td>CUDA Graph</td>
                    <td>æ— ç›´æ¥èŠ‚çœ</td>
                    <td>å‡å°‘å»¶è¿Ÿ</td>
                    <td>ä¸­</td>
                </tr>
            </table>

            <h3>5. Recompute (é‡è®¡ç®—)</h3>
            <div class="info-box warning">
                <h4>åå‘ä¼ æ’­ä¼˜åŒ–</h4>
                <p>åœ¨åå‘ä¼ æ’­æ—¶é‡è®¡ç®— attention è€Œéå­˜å‚¨ä¸­é—´ç»“æœï¼Œç‰ºç‰²è®¡ç®—æ—¶é—´æ¢å–å†…å­˜ã€‚</p>
                <pre># æ ‡å‡† Attention: å­˜å‚¨ O, L, S (softmax å‰å)
# Flash Attention: åªå­˜å‚¨ Oï¼Œé‡è®¡ç®— L å’Œ S</pre>
            </div>
        </div>

        <!-- Flash Attention MLA -->
        <div id="fa-mla" class="tab-content">
            <h2>MLA å’Œé«˜çº§ç‰¹æ€§</h2>

            <h3>MLA (Multi-Head Latent Attention)</h3>
            <div class="info-box success">
                <h4>æ ¸å¿ƒæ€æƒ³</h4>
                <p>MLA é€šè¿‡å‹ç¼© KV cache æ¥å¤§å¹…å‡å°‘å†…å­˜ä½¿ç”¨ï¼Œç‰¹åˆ«é€‚åˆ DeepSeek-V3 ç­‰å¤§æ¨¡å‹ã€‚</p>
                <p><strong>å†…å­˜èŠ‚çœ:</strong> å‹ç¼© KV å¯å‡å°‘ 90%+ çš„ KV cache å†…å­˜</p>
            </div>

            <h3>MLA æ¶æ„</h3>
            <div class="mermaid">
flowchart TD
    Input[Hidden States] --> Proj[æŠ•å½±å±‚]

    Proj --> QP[QæŠ•å½±åˆ°æ½œåœ¨ç©ºé—´]
    Proj --> KP[KVå‹ç¼©æŠ•å½±åˆ°ä½ç»´]

    KP --> KC[Compressed KVä½ç»´åº¦]
    KC --> Cache[MLA KV Cache]

    QP --> QL[Latent Query]
    Cache --> KL[Latent KV]

    QL --> UPQ[ä¸ŠæŠ•å½±Q]
    KL --> UPKV[ä¸ŠæŠ•å½±KV]

    UPQ --> Attn[Attention]
    UPKV --> Attn

    Attn --> Output[è¾“å‡º]

    style Cache fill:#58a6ff,color:#fff
            </div>

            <h3>æ ‡å‡† Attention vs MLA</h3>
            <table>
                <tr>
                    <th>ç‰¹æ€§</th>
                    <th>æ ‡å‡† Attention</th>
                    <th>MLA</th>
                </tr>
                <tr>
                    <td>KV ç»´åº¦</td>
                    <td><code>num_kv_heads Ã— head_size</code></td>
                    <td><code>num_latent_heads Ã— latent_size</code> (å‹ç¼©)</td>
                </tr>
                <tr>
                    <td>KV Cache å¤§å°</td>
                    <td>å¤§</td>
                    <td>å° (å‹ç¼©å)</td>
                </tr>
                <tr>
                    <td>è®¡ç®—å¤æ‚åº¦</td>
                    <td>O(LÂ²d)</td>
                    <td>O(LÂ²d_latent + additional_proj)</td>
                </tr>
                <tr>
                    <td>é€‚ç”¨æ¨¡å‹</td>
                    <td>å¤§å¤šæ•° LLM</td>
                    <td>DeepSeek-V3, Qwen, ç­‰</td>
                </tr>
            </table>

            <h3>Sparse MLA (ç¨€ç– MLA)</h3>
            <div class="info-box">
                <h4>ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶</h4>
                <p>å¯¹äºè¶…é•¿åºåˆ—ï¼Œåªè®¡ç®—éƒ¨åˆ†ä½ç½®çš„æ³¨æ„åŠ›ï¼Œè¿›ä¸€æ­¥å‡å°‘è®¡ç®—é‡ã€‚</p>
                <pre># æ”¯æŒ MLA çš„ç¨€ç–å˜ä½“
backends = ["FLASHMLA_SPARSE", "ROCMAITER_MLA_SPARSE"]</pre>
            </div>

            <h3>Cascade Attention (çº§è”æ³¨æ„åŠ›)</h3>
            <div class="mermaid">
flowchart TD
    Req1[Request1_Hello_world] --> FA1[Flash Attention]
    Req2[Request2_Hello] --> FA2[å…±äº«å‰ç¼€]

    Req3[Request3_Hello_there] --> FA3[å…±äº«å‰ç¼€]

    FA1 --> Cache1[Cache_KV_for_Hello_world]
    FA2 --> Cache2[å¤ç”¨Hello_KV_compute_world]
    FA3 --> Cache3[å¤ç”¨Hello_KV_compute_there]

    Cache1 --> Shared[å…±äº«å‰ç¼€Cache]
    Cache2 --> Shared
    Cache3 --> Shared

    style Shared fill:#58a6ff,color:#fff
            </div>

            <div class="info-box">
                <h4>ç”¨é€”</h4>
                <p>é«˜æ•ˆå¤„ç†å…·æœ‰å…±äº«å‰ç¼€çš„å¤šè¯·æ±‚ï¼Œå¦‚æ‰¹é‡æ¨ç†ã€å¯¹è¯ç³»ç»Ÿç­‰åœºæ™¯ã€‚</p>
            </div>

            <h3>Attention Sinks (æ³¨æ„åŠ›é”šç‚¹)</h3>
            <div class="info-box warning">
                <h4>é—®é¢˜</h4>
                <p>è¶…é•¿åºåˆ—ç”Ÿæˆæ—¶ï¼Œæ—©æœŸ token çš„æ³¨æ„åŠ›æƒé‡è¿‡ä½ï¼Œå¯¼è‡´ç”Ÿæˆè´¨é‡ä¸‹é™ã€‚</p>

                <h4>è§£å†³æ–¹æ¡ˆ</h4>
                <p>æ·»åŠ ç‰¹æ®Šçš„ sink tokensï¼Œå¼ºåˆ¶æ¨¡å‹å…³æ³¨è¿™äº›é”šç‚¹ï¼Œä¿æŒç”Ÿæˆè¿è´¯æ€§ã€‚</p>
                <pre># å¯ç”¨ attention sinks
use_attention_sinks = True
num_sink_tokens = 4  # é€šå¸¸ 4-16 ä¸ª sink tokens</pre>
            </div>

            <h3>Sliding Window Attention (æ»‘åŠ¨çª—å£)</h3>
            <div class="mermaid">
flowchart LR
    T[Token_t] --> W[æ»‘åŠ¨çª—å£W]

    W --> Curr[å½“å‰çª—å£_t_W_1_to_t]
    Curr --> Attn[åªè®¡ç®—çª—å£å†…æ³¨æ„åŠ›]

    Past[çª—å£å¤–çš„tokens] --> Skip[è·³è¿‡è®¡ç®—]

    style Attn fill:#58a6ff,color:#fff
            </div>

            <div class="data-structure">
                <p><strong>é…ç½®:</strong></p>
                <pre># å¯ç”¨æ»‘åŠ¨çª—å£
sliding_window = 4096  # çª—å£å¤§å°</pre>

                <p><strong>æ”¯æŒçš„åç«¯:</strong></p>
                <ul>
                    <li>FLASH_ATTN: æ”¯æŒ</li>
                    <li>FLASHINFER: æ”¯æŒ</li>
                    <li>TRITON_ATTN: æ”¯æŒ</li>
                </ul>
            </div>

            <h3>é«˜çº§ç‰¹æ€§å¯¹æ¯”</h3>
            <table>
                <tr>
                    <th>ç‰¹æ€§</th>
                    <th>æè¿°</th>
                    <th>æ”¯æŒçš„åç«¯</th>
                    <th>æ€§èƒ½å½±å“</th>
                </tr>
                <tr>
                    <td>MLA</td>
                    <td>å‹ç¼© KV cache</td>
                    <td>FLASHMLA, FLASHINFER_MLA</td>
                    <td>èŠ‚çœå†…å­˜ï¼Œè½»å¾®è®¡ç®—å¢åŠ </td>
                </tr>
                <tr>
                    <td>Sparse MLA</td>
                    <td>ç¨€ç–æ³¨æ„åŠ›</td>
                    <td>FLASHMLA_SPARSE</td>
                    <td>å¤§å¹…å‡å°‘è®¡ç®—</td>
                </tr>
                <tr>
                    <td>Cascade</td>
                    <td>å…±äº«å‰ç¼€ä¼˜åŒ–</td>
                    <td>FLASH_ATTN, FLASHINFER</td>
                    <td>æ˜¾è‘—æå‡å…±äº«å‰ç¼€åœºæ™¯</td>
                </tr>
                <tr>
                    <td>Sinks</td>
                    <td>æ³¨æ„åŠ›é”šç‚¹</td>
                    <td>FLASH_ATTN (TRTLLM), FLASHINFER</td>
                    <td>è½»å¾®è®¡ç®—å¢åŠ </td>
                </tr>
                <tr>
                    <td>Sliding Window</td>
                    <td>æ»‘åŠ¨çª—å£</td>
                    <td>æ‰€æœ‰åç«¯</td>
                    <td>çº¿æ€§å¤æ‚åº¦</td>
                </tr>
            </table>
        </div>

        <!-- Flash Attention Code Mapping -->
        <div id="fa-code" class="tab-content">
            <h2>ä»£ç æ–‡ä»¶æ˜ å°„</h2>

            <h3>æ ¸å¿ƒåç«¯æ–‡ä»¶</h3>
            <table>
                <tr>
                    <th>ç»„ä»¶</th>
                    <th>æ–‡ä»¶è·¯å¾„</th>
                    <th>å…³é”®ç±»/å‡½æ•°</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><span class="badge purple">Base Backend</span></td>
                    <td><code>vllm/v1/attention/backend.py</code></td>
                    <td>
                        <code>AttentionBackend</code><br>
                        <code>AttentionImpl</code><br>
                        <code>AttentionMetadataBuilder</code>
                    </td>
                    <td>æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰åç«¯æ¥å£</td>
                </tr>
                <tr>
                    <td><span class="badge blue">Flash Attention</span></td>
                    <td><code>vllm/v1/attention/backends/flash_attn.py</code></td>
                    <td>
                        <code>FlashAttentionBackend</code><br>
                        <code>FlashAttentionImpl</code><br>
                        <code>FlashAttentionMetadataBuilder</code>
                    </td>
                    <td>Flash Attention 2/3 å®ç°</td>
                </tr>
                <tr>
                    <td><span class="badge green">FlashInfer</span></td>
                    <td><code>vllm/v1/attention/backends/flashinfer.py</code></td>
                    <td>
                        <code>FlashInferBackend</code><br>
                        <code>FlashInferImpl</code><br>
                        <code>FlashInferMetadataBuilder</code>
                    </td>
                    <td>FlashInfer é«˜æ€§èƒ½å®ç°</td>
                </tr>
                <tr>
                    <td><span class="badge orange">Triton</span></td>
                    <td><code>vllm/v1/attention/backends/triton_attn.py</code></td>
                    <td>
                        <code>TritonAttentionBackend</code><br>
                        <code>TritonAttentionImpl</code>
                    </td>
                    <td>çº¯ Triton å®ç°</td>
                </tr>
                <tr>
                    <td><span class="badge blue">Selector</span></td>
                    <td><code>vllm/v1/attention/selector.py</code></td>
                    <td>
                        <code>get_attn_backend()</code><br>
                        <code>validate_and_select_backend()</code>
                    </td>
                    <td>åç«¯é€‰æ‹©é€»è¾‘</td>
                </tr>
                <tr>
                    <td><span class="badge purple">Registry</span></td>
                    <td><code>vllm/v1/attention/backends/registry.py</code></td>
                    <td>
                        <code>@register_attention_backend</code><br>
                        <code>get_all_backends()</code>
                    </td>
                    <td>åç«¯æ³¨å†Œè¡¨</td>
                </tr>
            </table>

            <h3>MLA ç›¸å…³æ–‡ä»¶</h3>
            <table>
                <tr>
                    <th>ç»„ä»¶</th>
                    <th>æ–‡ä»¶è·¯å¾„</th>
                    <th>å…³é”®ç±»</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><span class="badge blue">MLA Common</span></td>
                    <td><code>vllm/v1/attention/backends/mla/common.py</code></td>
                    <td><code>MLAImplBase</code></td>
                    <td>MLA åŸºç¡€ç»„ä»¶</td>
                </tr>
                <tr>
                    <td><span class="badge green">Flash MLA</span></td>
                    <td><code>vllm/v1/attention/backends/mla/flashmla.py</code></td>
                    <td><code>FlashMLABackend</code></td>
                    <td>Flash Attention MLA</td>
                </tr>
                <tr>
                    <td><span class="badge orange">FlashInfer MLA</span></td>
                    <td><code>vllm/v1/attention/backends/mla/flashinfer_mla.py</code></td>
                    <td><code>FlashInferMLABackend</code></td>
                    <td>FlashInfer MLA</td>
                </tr>
            </table>

            <h3>æ“ä½œç®—å­æ–‡ä»¶</h3>
            <table>
                <tr>
                    <th>ç»„ä»¶</th>
                    <th>æ–‡ä»¶è·¯å¾„</th>
                    <th>å…³é”®å‡½æ•°</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><span class="badge blue">Unified Attention</span></td>
                    <td><code>vllm/v1/attention/ops/triton_unified_attention.py</code></td>
                    <td><code>triton_flash_attention()</code></td>
                    <td>ç»Ÿä¸€çš„ Triton attention kernel</td>
                </tr>
                <tr>
                    <td><span class="badge green">Prefill</span></td>
                    <td><code>vllm/v1/attention/ops/triton_prefill_attention.py</code></td>
                    <td><code>prefill_attention()</code></td>
                    <td>Prefill ä¸“ç”¨ kernel</td>
                </tr>
                <tr>
                    <td><span class="badge orange">Decode</span></td>
                    <td><code>vllm/v1/attention/ops/triton_decode_attention.py</code></td>
                    <td><code>decode_attention()</code></td>
                    <td>Decode ä¸“ç”¨ kernel</td>
                </tr>
                <tr>
                    <td><span class="badge purple">Paged Attention</span></td>
                    <td><code>vllm/v1/attention/ops/paged_attn.py</code></td>
                    <td><code>reshape_and_cache_flash()</code></td>
                    <td>Paged KV cache æ“ä½œ</td>
                </tr>
            </table>

            <h3>CUDA å†…æ ¸æ–‡ä»¶</h3>
            <table>
                <tr>
                    <th>ç»„ä»¶</th>
                    <th>æ–‡ä»¶è·¯å¾„</th>
                    <th>å…³é”®å‡½æ•°</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><span class="badge blue">Paged Attn v1</span></td>
                    <td><code>csrc/attention/paged_attention_v1.cu</code></td>
                    <td><code>paged_attention_v1_kernel</code></td>
                    <td>Paged attention kernel v1</td>
                </tr>
                <tr>
                    <td><span class="badge green">Paged Attn v2</span></td>
                    <td><code>csrc/attention/paged_attention_v2.cu</code></td>
                    <td><code>paged_attention_v2_kernel</code></td>
                    <td>Paged attention kernel v2 (ä¼˜åŒ–ç‰ˆ)</td>
                </tr>
            </table>

            <h3>é…ç½®æ–‡ä»¶</h3>
            <table>
                <tr>
                    <th>ç»„ä»¶</th>
                    <th>æ–‡ä»¶è·¯å¾„</th>
                    <th>å…³é”®ç±»</th>
                    <th>åŠŸèƒ½</th>
                </tr>
                <tr>
                    <td><span class="badge blue">Config</span></td>
                    <td><code>vllm/config/attention.py</code></td>
                    <td><code>AttentionConfig</code></td>
                    <td>Attention é…ç½®</td>
                </tr>
                <tr>
                    <td><span class="badge purple">Cache Config</span></td>
                    <td><code>vllm/config/cache.py</code></td>
                    <td><code>CacheConfig</code></td>
                    <td>KV cache é…ç½®</td>
                </tr>
            </table>

            <h3>æ–‡ä»¶ä¾èµ–å…³ç³»</h3>
            <div class="mermaid">
graph TD
    Config[config/attention.py] --> Selector[v1/attention/selector.py]
    Registry[v1/attention/backends/registry.py] --> Selector

    FA_Backend[v1/attention/backends/flash_attn.py] --> Registry
    FI_Backend[v1/attention/backends/flashinfer.py] --> Registry
    TR_Backend[v1/attention/backends/triton_attn.py] --> Registry

    MLA_Flash[v1/attention/backends/mla/flashmla.py] --> FA_Backend
    MLA_FI[v1/attention/backends/mla/flashinfer_mla.py] --> FI_Backend

    Ops[v1/attention/ops/] --> FA_Backend
    Ops --> FI_Backend
    Ops --> TR_Backend

    CUDA[csrc/attention/] --> Ops

    style Config fill:#d29922
    style Selector fill:#a371f7,color:#fff
    style Registry fill:#58a6ff,color:#fff
            </div>

            <div class="info-box">
                <h4>å¿«é€Ÿå¯¼èˆª</h4>
                <p>ğŸ“ <strong>æ ¸å¿ƒåç«¯:</strong> <code>vllm/v1/attention/backends/</code></p>
                <p>ğŸ“ <strong>æ“ä½œç®—å­:</strong> <code>vllm/v1/attention/ops/</code></p>
                <p>ğŸ“ <strong>CUDA å†…æ ¸:</strong> <code>csrc/attention/</code></p>
                <p>ğŸ“ <strong>é…ç½®:</strong> <code>vllm/config/attention.py</code></p>
            </div>
        </div>
    </div>

    <script>
        mermaid.initialize({
            startOnLoad: true,
            theme: 'dark',
            themeVariables: {
                primaryColor: '#58a6ff',
                primaryTextColor: '#fff',
                primaryBorderColor: '#58a6ff',
                lineColor: '#8b949e',
                secondaryColor: '#21262d',
                tertiaryColor: '#161b22',
                background: '#0d1117',
                mainBkg: '#21262d',
                nodeBorder: '#30363d',
                clusterBkg: '#161b22',
                titleColor: '#e6edf3',
                edgeLabelBackground: '#0d1117',
            }
        });

        function showTab(tabName) {
            // Hide all tabs
            const tabs = document.querySelectorAll('.tab-content');
            tabs.forEach(tab => tab.classList.remove('active'));

            // Remove active from all buttons
            const buttons = document.querySelectorAll('.tab');
            buttons.forEach(btn => btn.classList.remove('active'));

            // Show selected tab
            document.getElementById(tabName).classList.add('active');

            // Add active to clicked button
            event.target.classList.add('active');

            // Re-render mermaid diagrams
            mermaid.init(undefined, document.querySelectorAll('#' + tabName + ' .mermaid'));
        }

        // Initialize all mermaid diagrams on load
        document.addEventListener('DOMContentLoaded', function() {
            mermaid.init();
        });
    </script>
</body>
</html>
