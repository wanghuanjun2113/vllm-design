# vLLM Blend æ’ä»¶ç‹¬ç«‹åŒ–è®¾è®¡æ–¹æ¡ˆ

## æ–‡æ¡£ç‰ˆæœ¬

- **ç‰ˆæœ¬**: v1.0
- **æ—¥æœŸ**: 2025-01-13
- **çŠ¶æ€**: è®¾è®¡é˜¶æ®µ

## 1. é¡¹ç›®æ¦‚è¿°

### 1.1 èƒŒæ™¯

LMCache æä¾›äº† CacheBlend åŠŸèƒ½ï¼Œå…è®¸åœ¨éå‰ç¼€ä½ç½®å¤ç”¨ KV cacheï¼Œç‰¹åˆ«é€‚åˆ RAGã€å¤šæ–‡æ¡£é—®ç­”ç­‰åœºæ™¯ã€‚ç›®å‰ Blend åŠŸèƒ½æ·±åº¦è€¦åˆåœ¨ LMCache ä¸­ï¼Œé™åˆ¶äº†å…¶ä½¿ç”¨èŒƒå›´ã€‚

### 1.2 ç›®æ ‡

å°† Blend åŠŸèƒ½ä» LMCache ä¸­æŠ½ç¦»å‡ºæ¥ï¼Œè®¾è®¡ä¸€ä¸ªç‹¬ç«‹çš„ vLLM æ’ä»¶ï¼Œå®ç°ï¼š

1. **å®Œå…¨è§£è€¦**: ç§»é™¤å¯¹ LMCache çš„æ‰€æœ‰ä¾èµ–
2. **ç¡¬ä»¶æ— å…³**: æ”¯æŒ CUDAã€Ascend NPUã€ROCm ç­‰å¤šç§ç¡¬ä»¶å¹³å°
3. **æ’ä»¶åŒ–é›†æˆ**: é€šè¿‡ vLLM çš„æ’ä»¶ç³»ç»Ÿæ³¨å†Œ
4. **å‘åå…¼å®¹**: ä¸ vLLM v1 ä¿æŒå®Œå…¨å…¼å®¹
5. **æ˜“äºæ‰©å±•**: æ–¹ä¾¿æ·»åŠ æ–°çš„æ¨¡å‹å’Œç¡¬ä»¶æ”¯æŒ

### 1.3 é€‚ç”¨åœºæ™¯

- **RAG (æ£€ç´¢å¢å¼ºç”Ÿæˆ)**: å¤ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ KV cache
- **å¤šæ–‡æ¡£é—®ç­”**: ä¸åŒé¡ºåºç»„åˆç›¸åŒæ–‡æ¡£æ—¶å¤ç”¨ KV cache
- **é•¿ä¸Šä¸‹æ–‡å¤„ç†**: åˆ†æ®µå¤„ç†é•¿æ–‡æ¡£ï¼Œå‡å°‘é‡å¤è®¡ç®—
- **å¯¹è¯ç³»ç»Ÿ**: å¤ç”¨å¯¹è¯å†å²ï¼Œä»…é‡ç®—æ–°é—®é¢˜éƒ¨åˆ†

## 2. æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         vLLM v1 Engine                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              Blend Plugin (vllm-blend)               â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚ BlendWorker â”‚â—„â”€â”€â”¤  BlendModelRunnerMixin   â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚         â”‚                          â”‚                â”‚   â”‚
â”‚  â”‚         â–¼                          â–¼                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚            BlendBlender (Core)               â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  Selector â”‚  â”‚ Metadata â”‚  â”‚  Utils  â”‚  â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚         â–²                â–²                â–²        â”‚   â”‚
â”‚  â”‚         â”‚                â”‚                â”‚        â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚  â”‚   Cache     â”‚  â”‚    GPU    â”‚  â”‚   Model    â”‚   â”‚   â”‚
â”‚  â”‚  â”‚  Provider   â”‚  â”‚ Provider  â”‚  â”‚  Provider  â”‚   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â”‚         â”‚                â”‚                â”‚        â”‚   â”‚
â”‚  â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚                          â–¼                         â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚   â”‚
â”‚  â”‚  â”‚          Backend Implementations             â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â”‚  CUDA   â”‚ â”‚   NPU   â”‚ â”‚    ROCm     â”‚    â”‚ â”‚   â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚   â”‚
â”‚  â”‚                                                     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         Model Adapters (Pluggable)                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚   â”‚
â”‚  â”‚  â”‚ Llama   â”‚ â”‚  Qwen   â”‚ â”‚ Mistral â”‚  ...         â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ ¸å¿ƒç»„ä»¶

#### 2.2.1 Provider æŠ½è±¡å±‚

**è®¾è®¡ç†å¿µ**: é€šè¿‡æŠ½è±¡æ¥å£å®ç°ä¸å…·ä½“å®ç°çš„è§£è€¦ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶å’Œç¼“å­˜åç«¯ã€‚

**ä¸‰ä¸ªæ ¸å¿ƒæ¥å£**:

```python
# CacheProviderInterface - KV ç¼“å­˜æä¾›è€…
class CacheProviderInterface(ABC):
    """æŠ½è±¡ KV ç¼“å­˜è®¿é—®æ¥å£"""

    @abstractmethod
    def retrieve_layer(
        self,
        tokens: torch.Tensor,
        layer_id: int,
        metadata: dict,
    ) -> Optional[tuple[torch.Tensor, torch.Tensor]]:
        """ä»ç¼“å­˜ä¸­æ£€ç´¢æŒ‡å®šå±‚çš„ KV tensors

        Returns:
            (k_cache, v_cache) æˆ– Noneï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        """
        pass

    @abstractmethod
    def store_layer(
        self,
        tokens: torch.Tensor,
        layer_id: int,
        k_cache: torch.Tensor,
        v_cache: torch.Tensor,
        metadata: dict,
    ) -> None:
        """å­˜å‚¨ KV tensors åˆ°ç¼“å­˜"""
        pass

    @abstractmethod
    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‘½ä¸­ç‡ç­‰ï¼‰"""
        pass

# GPUProviderInterface - GPU KV è®¿é—®
class GPUProviderInterface(ABC):
    """æŠ½è±¡ GPU KV cache è®¿é—®æ¥å£"""

    @abstractmethod
    def get_kv(self, layer_id: int) -> tuple[torch.Tensor, torch.Tensor]:
        """ä» GPU å†…å­˜è·å–å½“å‰ KV tensors"""
        pass

    @abstractmethod
    def update_kv(
        self,
        layer_id: int,
        k_update: torch.Tensor,
        v_update: torch.Tensor,
        indices: torch.Tensor,
    ) -> None:
        """æ›´æ–° GPU KV cache ä¸­æŒ‡å®šä½ç½®çš„å€¼"""
        pass

    @abstractmethod
    def get_kv_shape(self) -> tuple:
        """è·å– KV cache tensor çš„å½¢çŠ¶"""
        pass

# ModelProviderInterface - æ¨¡å‹è®¡ç®—
class ModelProviderInterface(ABC):
    """æŠ½è±¡æ¨¡å‹è®¿é—®æ¥å£"""

    @abstractmethod
    def get_num_layers(self) -> int:
        """è·å–æ¨¡å‹å±‚æ•°"""
        pass

    @abstractmethod
    def compute_layer_qkv(
        self,
        layer_id: int,
        hidden_states: torch.Tensor,
        residual: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """è®¡ç®—æŒ‡å®šå±‚çš„ QKV projection

        Returns:
            (q, k, v, residual)
        """
        pass

    @abstractmethod
    def apply_rotary_emb(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        positions: torch.Tensor,
        layer_id: int,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """åº”ç”¨ rotary position encoding"""
        pass
```

#### 2.2.2 BlendBlender æ ¸å¿ƒé€»è¾‘

```python
class BlendBlender:
    """æ ¸å¿ƒæ··åˆé€»è¾‘ - å®Œå…¨è§£è€¦çš„å®ç°"""

    def __init__(
        self,
        cache_provider: CacheProviderInterface,
        gpu_provider: GPUProviderInterface,
        model_provider: ModelProviderInterface,
        common_metadata: BlendCommonMetadata,
    ):
        self.cache_provider = cache_provider
        self.gpu_provider = gpu_provider
        self.model_provider = model_provider
        self.common_metadata = common_metadata

        self.metadata = BlendMetadata()
        self.selector = TokenSelector(common_metadata)

    def process_qkv(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        residual: torch.Tensor,
        layer_id: int,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
        """å¤„ç† QKV å¹¶æ‰§è¡Œæ··åˆ

        è¿™æ˜¯æ¯ä¸ªå±‚æ‰§è¡Œæ—¶è°ƒç”¨çš„æ ¸å¿ƒæ–¹æ³•
        """
        # 1. å°è¯•ä»ç¼“å­˜è·å– KV
        cached_k, cached_v = self.cache_provider.retrieve_layer(...)

        if cached_k is None:
            return q, k, v, residual  # ç¼“å­˜æœªå‘½ä¸­

        # 2. è·å– GPU ä¸­çš„ KVï¼ˆç”¨äºæ¯”è¾ƒï¼‰
        gpu_k, gpu_v = self.gpu_provider.get_kv(layer_id)

        # 3. åº”ç”¨ rotary embedding
        q, k = self.model_provider.apply_rotary_emb(...)

        # 4. å¦‚æœæ˜¯æ£€æŸ¥å±‚ï¼Œæ‰§è¡Œæ··åˆ
        if layer_id in self.common_metadata.check_layers:
            imp_indices = self.selector.select_important_tokens(
                new_k=k, old_k=gpu_k,
                ratio=self.common_metadata.recomp_ratios[0]
            )

            # æ›´æ–° GPU cache
            self.gpu_provider.update_kv(
                layer_id=layer_id,
                k_update=k[imp_indices],
                v_update=v[imp_indices],
                indices=imp_indices,
            )

            # åªè¿”å›é€‰ä¸­çš„ tokens
            return q[imp_indices], gpu_k, gpu_v, residual[imp_indices]

        # 5. éæ£€æŸ¥å±‚ç›´æ¥è¿”å›
        return q, k, v, residual
```

#### 2.2.3 TokenSelector é€‰æ‹©ç®—æ³•

```python
class TokenSelector:
    """é€‰æ‹©éœ€è¦é‡æ–°è®¡ç®—çš„é‡è¦ tokens"""

    def select_important_tokens(
        self,
        new_k: torch.Tensor,  # æ–°è®¡ç®—çš„ K
        old_k: torch.Tensor,  # ç¼“å­˜çš„ K
        ratio: float,        # é‡ç®—æ¯”ä¾‹
    ) -> torch.Tensor:
        """åŸºäº L2 è·ç¦»é€‰æ‹© top-K tokens

        ç®—æ³•ï¼š
        1. è®¡ç®—æ–°æ—§ K çš„ L2 è·ç¦»ï¼ˆåœ¨ heads å’Œ head_dim ç»´åº¦ä¸Šå¹³å‡ï¼‰
        2. é€‰æ‹©è·ç¦»æœ€å¤§çš„ top-K tokens
        3. è¿”å›æ’åºåçš„ç´¢å¼•
        """
        # è®¡ç®—å·®å¼‚
        diff_k = torch.sum(
            (new_k.to(torch.float32) - old_k.to(torch.float32)) ** 2,
            dim=[1, 2],  # åœ¨ heads å’Œ head_dim ä¸Šæ±‚å’Œ
        )

        total_len = diff_k.shape[0]
        topk_num = max(int(total_len * ratio), 1)

        # è·å– top-k ç´¢å¼•
        top_indices = torch.topk(diff_k, k=topk_num).indices
        top_indices, _ = torch.sort(top_indices)

        return top_indices
```

#### 2.2.4 æ¨¡å‹é€‚é…å™¨ç³»ç»Ÿ

```python
class BaseModelAdapter(ModelProviderInterface):
    """æ¨¡å‹é€‚é…å™¨åŸºç±»"""

    def __init__(self, vllm_model):
        self.vllm_model = vllm_model
        self.num_layers = len(vllm_model.model.layers)

    @abstractmethod
    def extract_qkv_from_layer(
        self,
        layer_id: int,
        hidden_states: torch.Tensor,
    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """ä»å±‚ä¸­æå– QKV projection - æ¨¡å‹ç‰¹å®šå®ç°"""
        pass

# Llama é€‚é…å™¨ç¤ºä¾‹
class LlamaAdapter(BaseModelAdapter):
    """Llama æ¨¡å‹ç‰¹å®šé€‚é…å™¨"""

    def extract_qkv_from_layer(self, layer_id, hidden_states):
        layer = self.vllm_model.model.layers[layer_id]

        # QKV projection
        qkv, _ = layer.self_attn.qkv_proj(hidden_states)
        q, k, v = qkv.split([
            layer.self_attn.num_q_heads * layer.self_attn.head_dim,
            layer.self_attn.num_kv_heads * layer.self_attn.head_dim,
            layer.self_attn.num_kv_heads * layer.self_attn.head_dim,
        ], dim=-1)

        return q, k, v

    def get_rotary_emb(self, layer_id):
        return self.vllm_model.model.layers[layer_id].self_attn.rotary_emb

    def apply_rotary_emb(self, q, k, positions, layer_id):
        rotary_emb = self.get_rotary_emb(layer_id)
        return rotary_emb(positions, q, k)
```

### 2.3 æ’ä»¶é›†æˆ

#### 2.3.1 å¹³å°æ³¨å†Œ

```python
class BlendPlatform(Platform):
    """Blend å¹³å°æ’ä»¶"""

    _enum = PlatformEnum.OOT
    device_name = "blend"
    device_type = "blend"  # ä¸å®é™…ä½¿ç”¨ï¼Œä»…ç”¨äºæ ‡è¯†

    @classmethod
    def check_and_update_config(cls, vllm_config: VllmConfig) -> None:
        """é›†æˆ Blend é…ç½®åˆ° vLLM"""
        from vllm_blend.config import BlendConfig

        blend_config = BlendConfig.from_vllm_config(vllm_config)

        if blend_config.enabled:
            # å­˜å‚¨é…ç½®
            vllm_config.additional_config["blend_config"] = blend_config

            # è®¾ç½® Worker ç±»
            if vllm_config.parallel_config.worker_cls == "auto":
                vllm_config.parallel_config.worker_cls = (
                    "vllm_blend.worker.blend_worker.BlendWorker"
                )

    @classmethod
    def get_attn_backend_cls(cls, selected_backend, attn_selector_config):
        """è·å–æ³¨æ„åŠ›åç«¯ï¼Œå¯èƒ½åŒ…è£…ä»¥æ”¯æŒ Blend"""
        # è·å–åº•å±‚å¹³å°çš„åç«¯
        base_backend = cls._get_base_platform().get_attn_backend_cls(
            selected_backend, attn_selector_config
        )

        # å¦‚æœå¯ç”¨ Blendï¼ŒåŒ…è£…åç«¯
        if cls._is_blend_enabled():
            from vllm_blend.backends import wrap_attention_for_blend
            return wrap_attention_for_blend(base_backend)

        return base_backend
```

#### 2.3.2 Worker é›†æˆ

```python
class BlendWorker(GPUWorker):
    """æ”¯æŒ Blend çš„ Worker"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        # åˆå§‹åŒ– Blender
        blend_config = self.vllm_config.additional_config.get("blend_config")
        if blend_config and blend_config.enabled:
            self.blender = self._init_blender(blend_config)
        else:
            self.blender = None

    def _init_blender(self, blend_config):
        """åˆå§‹åŒ– BlendBlender åŠå…¶ providers"""
        from vllm_blend.backends import get_providers_for_device

        cache_provider = get_providers_for_device(
            provider_type="cache",
            device=self.device,
            config=blend_config,
        )
        gpu_provider = get_providers_for_device(
            provider_type="gpu",
            device=self.device,
            model_runner=self.model_runner,
        )
        model_provider = get_providers_for_device(
            provider_type="model",
            device=self.device,
            vllm_model=self.model_runner.model,
        )

        from vllm_blend.core.blender import BlendBlender
        return BlendBlender(
            cache_provider=cache_provider,
            gpu_provider=gpu_provider,
            model_provider=model_provider,
            common_metadata=blend_config.common_metadata,
        )

    def execute_model(self, scheduler_output):
        """æ‰§è¡Œæ¨¡å‹ï¼ˆå¸¦ Blend æ”¯æŒï¼‰"""
        if self.blender is None:
            return super().execute_model(scheduler_output)

        # å°† blender æ³¨å…¥åˆ° model_runner
        return self.model_runner.execute_with_blend(
            scheduler_output,
            blender=self.blender,
        )
```

### 2.4 Backend å®ç°

#### 2.4.1 CUDA Backend

```python
class CUDAGPUProvider(GPUProviderInterface):
    """CUDA GPU KV cache provider"""

    def __init__(self, model_runner):
        self.model_runner = model_runner
        assert isinstance(model_runner, KVConnectorModelRunnerMixin)

    def get_kv(self, layer_id: int):
        """é€šè¿‡ KV connector è·å– KV"""
        return self.model_runner.get_kv_from_connector(layer_id)

    def update_kv(self, layer_id, k_update, v_update, indices):
        """æ›´æ–° GPU KV cache"""
        k_gpu, v_gpu = self.get_kv(layer_id)
        k_gpu[indices] = k_update
        v_gpu[indices] = v_update

class CUDACacheProvider(CacheProviderInterface):
    """CUDA ç¼“å­˜ providerï¼ˆå¯ä»¥åŒ…è£… LMCacheï¼‰"""

    def __init__(self, config):
        self.config = config
        # å¯ä»¥é€‰æ‹©ä½¿ç”¨ LMCacheã€CPU memory ç­‰

        if config.cache_provider == "lmcache":
            from lmcache.v1 import LMCacheEngine
            self.cache_engine = LMCacheEngine(...)
        else:
            self.cache_storage = {}  # ç®€å•å†…å­˜ç¼“å­˜

    def retrieve_layer(self, tokens, layer_id, metadata):
        if hasattr(self, 'cache_engine'):
            return self.cache_engine.retrieve_layer(tokens, layer_id, **metadata)
        else:
            key = (tuple(tokens.tolist()), layer_id)
            return self.cache_storage.get(key)
```

#### 2.4.2 NPU Backend (Ascend)

```python
class NPUGPUProvider(GPUProviderInterface):
    """Ascend NPU GPU KV cache provider"""

    def __init__(self, model_runner):
        self.model_runner = model_runner
        # NPU ç‰¹å®šåˆå§‹åŒ–

    def get_kv(self, layer_id: int):
        """ä» NPU å†…å­˜è·å– KV

        å®ç°å–å†³äº vLLM-Ascend çš„ KV ç®¡ç†æ–¹å¼
        å¯èƒ½é€šè¿‡ HCCL æˆ– NPU å†…å­˜è®¿é—®
        """
        # vLLM-Ascend ç‰¹å®šå®ç°
        pass

    def update_kv(self, layer_id, k_update, v_update, indices):
        """ä½¿ç”¨ NPU ä¼˜åŒ–æ“ä½œæ›´æ–° KV cache"""
        import torch_npu

        k_gpu, v_gpu = self.get_kv(layer_id)

        # ä½¿ç”¨ torch_npu é«˜æ•ˆæ›´æ–°
        torch_npu.copy_(k_gpu[indices], k_update)
        torch_npu.copy_(v_gpu[indices], v_update)

class NPUCacheProvider(CacheProviderInterface):
    """NPU ç¼“å­˜ provider"""

    def __init__(self, config):
        self.config = config
        # å¯ä»¥ä¸ vLLM-Ascend çš„ KV pool é›†æˆ

    def retrieve_layer(self, tokens, layer_id, metadata):
        # ä» Ascend KV pool æˆ–è¿œç¨‹å­˜å‚¨æ£€ç´¢
        pass
```

## 3. ç›®å½•ç»“æ„

```
vllm-blend/
â”œâ”€â”€ setup.py                      # æ’ä»¶å…¥å£å®šä¹‰
â”œâ”€â”€ README.md                     # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ requirements.txt              # ä¾èµ–
â”œâ”€â”€ pyproject.toml               # é¡¹ç›®é…ç½®
â”‚
â”œâ”€â”€ tests/                        # æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_blender.py          # æ ¸å¿ƒé€»è¾‘æµ‹è¯•
â”‚   â”œâ”€â”€ test_providers.py        # Provider æµ‹è¯•
â”‚   â”œâ”€â”€ test_adapters.py         # é€‚é…å™¨æµ‹è¯•
â”‚   â””â”€â”€ integration/             # é›†æˆæµ‹è¯•
â”‚       â”œâ”€â”€ test_cuda.py
â”‚       â””â”€â”€ test_npu.py
â”‚
â”œâ”€â”€ examples/                     # ç¤ºä¾‹
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ rag_example.py
â”‚   â””â”€â”€ multi_doc_qa.py
â”‚
â””â”€â”€ vllm_blend/                   # ä¸»ä»£ç 
    â”œâ”€â”€ __init__.py              # æ’ä»¶æ³¨å†Œ
    â”œâ”€â”€ config.py                # BlendConfig
    â”œâ”€â”€ platform.py              # BlendPlatform
    â”‚
    â”œâ”€â”€ core/                    # æ ¸å¿ƒé€»è¾‘
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ blender.py           # BlendBlender
    â”‚   â”œâ”€â”€ metadata.py          # å…ƒæ•°æ®
    â”‚   â””â”€â”€ selector.py          # TokenSelector
    â”‚
    â”œâ”€â”€ providers/               # æŠ½è±¡æ¥å£
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ cache_provider.py    # CacheProviderInterface
    â”‚   â”œâ”€â”€ gpu_provider.py      # GPUProviderInterface
    â”‚   â””â”€â”€ model_provider.py    # ModelProviderInterface
    â”‚
    â”œâ”€â”€ adapters/                # æ¨¡å‹é€‚é…å™¨
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ base.py              # BaseModelAdapter
    â”‚   â”œâ”€â”€ llama.py             # LlamaAdapter
    â”‚   â”œâ”€â”€ qwen2.py             # Qwen2Adapter
    â”‚   â”œâ”€â”€ qwen3.py             # Qwen3Adapter
    â”‚   â”œâ”€â”€ mistral.py           # MistralAdapter
    â”‚   â””â”€â”€ registry.py          # é€‚é…å™¨æ³¨å†Œè¡¨
    â”‚
    â”œâ”€â”€ backends/                # ç¡¬ä»¶å®ç°
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ factory.py           # Backend factory
    â”‚   â”‚
    â”‚   â”œâ”€â”€ cuda/                # CUDA å®ç°
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ gpu_provider.py
    â”‚   â”‚   â”œâ”€â”€ cache_provider.py
    â”‚   â”‚   â””â”€â”€ model_provider.py
    â”‚   â”‚
    â”‚   â”œâ”€â”€ npu/                 # Ascend NPU å®ç°
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â”œâ”€â”€ gpu_provider.py
    â”‚   â”‚   â”œâ”€â”€ cache_provider.py
    â”‚   â”‚   â”œâ”€â”€ model_provider.py
    â”‚   â”‚   â””â”€â”€ attention.py      # NPU ç‰¹å®šæ³¨æ„åŠ›ä¼˜åŒ–
    â”‚   â”‚
    â”‚   â””â”€â”€ rocm/                # ROCm å®ç°
    â”‚       â”œâ”€â”€ __init__.py
    â”‚       â”œâ”€â”€ gpu_provider.py
    â”‚       â””â”€â”€ cache_provider.py
    â”‚
    â”œâ”€â”€ worker/                  # Worker é›†æˆ
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ blend_worker.py      # BlendWorker
    â”‚   â””â”€â”€ model_runner.py      # BlendModelRunnerMixin
    â”‚
    â””â”€â”€ utils/                   # å·¥å…·
        â”œâ”€â”€ __init__.py
        â”œâ”€â”€ rope.py              # RoPE å·¥å…·
        â””â”€â”€ diagnostics.py       # è¯Šæ–­å’Œç›‘æ§
```

## 4. é…ç½®ç³»ç»Ÿ

### 4.1 BlendConfig

```python
@dataclass
class BlendConfig:
    """Blend é…ç½®"""

    # å¯ç”¨/ç¦ç”¨
    enabled: bool = False

    # æ£€æŸ¥å±‚åˆ—è¡¨
    check_layers: List[int] = field(default_factory=lambda: [0, 16, 32])

    # é‡ç®—æ¯”ä¾‹
    recompute_ratios: List[float] = field(default_factory=lambda: [0.1])

    # å†³ç­–é˜ˆå€¼ï¼ˆå¯é€‰ï¼‰
    thresholds: Optional[List[float]] = None

    # ç¼“å­˜åç«¯é€‰æ‹©
    cache_provider: str = "cpu"  # lmcache, cpu, remote

    # ç¼“å­˜é…ç½®
    cache_config: dict = field(default_factory=dict)

    def __post_init__(self):
        """éªŒè¯é…ç½®"""
        if not self.check_layers:
            raise ValueError("check_layers ä¸èƒ½ä¸ºç©º")

        for ratio in self.recompute_ratios:
            if not 0.0 <= ratio <= 1.0:
                raise ValueError(f"recompute_ratio å¿…é¡»åœ¨ [0, 1] ä¹‹é—´ï¼Œå¾—åˆ° {ratio}")

    @classmethod
    def from_vllm_config(cls, vllm_config: VllmConfig) -> "BlendConfig":
        """ä» VllmConfig åˆ›å»º"""
        additional_config = vllm_config.additional_config or {}
        blend_config_dict = additional_config.get("blend_config", {})
        return cls(**blend_config_dict)

    @property
    def common_metadata(self) -> BlendCommonMetadata:
        """è½¬æ¢ä¸º BlendCommonMetadata"""
        return BlendCommonMetadata(
            check_layers=self.check_layers,
            recomp_ratios=self.recompute_ratios,
            thresholds=self.thresholds,
        )
```

### 4.2 å‘½ä»¤è¡Œå‚æ•°

```python
def register_blend_config():
    """æ³¨å†Œ Blend å‚æ•°åˆ° vLLM argument parser"""
    def add_blend_args(parser):
        parser.add_argument(
            "--enable-blend",
            action="store_true",
            help="å¯ç”¨ Blend åŠŸèƒ½",
        )
        parser.add_argument(
            "--blend-check-layers",
            type=int,
            nargs="+",
            default=[0, 16, 32],
            help="æ‰§è¡Œæ··åˆæ£€æŸ¥çš„å±‚ç´¢å¼•",
        )
        parser.add_argument(
            "--blend-recompute-ratios",
            type=float,
            nargs="+",
            default=[0.1],
            help="æ¯å±‚é‡æ–°è®¡ç®—çš„ token æ¯”ä¾‹",
        )
        parser.add_argument(
            "--blend-cache-provider",
            type=str,
            default="cpu",
            choices=["cpu", "lmcache", "remote"],
            help="ç¼“å­˜åç«¯é€‰æ‹©",
        )

    # æ³¨å†Œåˆ° vLLM
    from vllm.cli.args import LooseArgumentParser
    LooseArgumentParser.register_argument_adder(add_blend_args)
```

## 5. ä½¿ç”¨æ–¹å¼

### 5.1 å‘½ä»¤è¡Œ

```bash
# åŸºç¡€ç”¨æ³•
vllm serve meta-llama/Llama-2-7b-chat-hf \
    --enable-blend

# è‡ªå®šä¹‰é…ç½®
vllm serve meta-llama/Llama-2-7b-chat-hf \
    --enable-blend \
    --blend-check-layers 0 8 16 24 32 \
    --blend-recompute-ratios 0.15

# ä½¿ç”¨ LMCache åç«¯
vllm serve meta-llama/Llama-2-7b-chat-hf \
    --enable-blend \
    --blend-cache-provider lmcache
```

### 5.2 Python API

```python
from vllm import LLM, SamplingParams

# åŸºç¡€ç”¨æ³•
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    enable_blend=True,
)

# è‡ªå®šä¹‰é…ç½®
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    enable_blend=True,
    blend_check_layers=[0, 16, 32],
    blend_recompute_ratios=[0.1],
)

# æ¨ç†
outputs = llm.generate("Hello, world!", SamplingParams(max_tokens=10))
```

### 5.3 RAG ç¤ºä¾‹

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–
llm = LLM(
    model="meta-llama/Llama-2-7b-chat-hf",
    enable_blend=True,
    blend_check_layers=[0],
    blend_recompute_ratios=[0.15],
)

# æ–‡æ¡£ chunks
documents = {
    "doc1": "Content of document 1...",
    "doc2": "Content of document 2...",
    "doc3": "Content of document 3...",
}

# å¤šä¸ªæŸ¥è¯¢ï¼Œä¸åŒé¡ºåº
queries = [
    (["doc1", "doc2", "doc3"], "What is the summary?"),
    (["doc3", "doc1", "doc2"], "Compare doc3 and doc1"),
    (["doc2", "doc3", "doc1"], "What are the key points?"),
]

for doc_order, question in queries:
    # æ„å»ºæç¤ºï¼ˆä½¿ç”¨ç‰¹æ®Šåˆ†éš”ç¬¦ï¼‰
    prompt = build_prompt_with_separator(documents, doc_order, question)

    # ç¬¬ä¸€æ¬¡è¯·æ±‚ä¼šè®¡ç®—å¹¶ç¼“å­˜
    # åç»­è¯·æ±‚ä¼šå¤ç”¨æ–‡æ¡£çš„ KV cacheï¼Œåªé‡ç®—è¿æ¥å¤„
    output = llm.generate(prompt, SamplingParams(max_tokens=100))
    print(output[0].outputs[0].text)
```

## 6. å®æ–½è®¡åˆ’

### Phase 1: æ ¸å¿ƒåŸºç¡€è®¾æ–½ (Week 1-2)

**ç›®æ ‡**: å®ç°æ ¸å¿ƒ Blend é€»è¾‘å’Œé…ç½®ç³»ç»Ÿ

**å…³é”®æ–‡ä»¶**:
- `vllm_blend/core/blender.py`
- `vllm_blend/core/metadata.py`
- `vllm_blend/core/selector.py`
- `vllm_blend/config.py`

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç°å…ƒæ•°æ®ç±»ï¼ˆBlendCommonMetadata, BlendMetadataï¼‰
- [ ] å®ç° TokenSelector ç®—æ³•
- [ ] å®ç° BlendBlenderï¼ˆä½¿ç”¨æŠ½è±¡ providersï¼‰
- [ ] å®ç° BlendConfig
- [ ] å®ç°å‚æ•°æ³¨å†Œç³»ç»Ÿ
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•

### Phase 2: Provider æŠ½è±¡å±‚ (Week 3)

**ç›®æ ‡**: å®ç°æŠ½è±¡æ¥å£å’Œ CUDA backend

**å…³é”®æ–‡ä»¶**:
- `vllm_blend/providers/*.py`
- `vllm_blend/backends/cuda/gpu_provider.py`
- `vllm_blend/backends/cuda/cache_provider.py`
- `vllm_blend/backends/cuda/model_provider.py`

**ä»»åŠ¡æ¸…å•**:
- [ ] å®šä¹‰ä¸‰ä¸ªæŠ½è±¡ provider æ¥å£
- [ ] å®ç° CUDA GPU provider
- [ ] å®ç° LMCache adapter ä½œä¸º cache provider
- [ ] å®ç° CPU cache providerï¼ˆç”¨äºæµ‹è¯•ï¼‰
- [ ] å®ç° CUDA model provider
- [ ] ç¼–å†™ provider æµ‹è¯•

### Phase 3: æ¨¡å‹é€‚é…å™¨ (Week 4)

**ç›®æ ‡**: å®ç°æ¨¡å‹é€‚é…å™¨ç³»ç»Ÿ

**å…³é”®æ–‡ä»¶**:
- `vllm_blend/adapters/base.py`
- `vllm_blend/adapters/llama.py`
- `vllm_blend/adapters/qwen2.py`
- `vllm_blend/adapters/qwen3.py`
- `vllm_blend/adapters/registry.py`

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° BaseModelAdapter
- [ ] å®ç° LlamaAdapter
- [ ] å®ç° Qwen2Adapter
- [ ] å®ç° Qwen3Adapter
- [ ] å®ç°é€‚é…å™¨æ³¨å†Œè¡¨
- [ ] ç¼–å†™é€‚é…å™¨æµ‹è¯•

### Phase 4: Worker é›†æˆ (Week 5)

**ç›®æ ‡**: é›†æˆåˆ° vLLM Worker

**å…³é”®æ–‡ä»¶**:
- `vllm_blend/worker/blend_worker.py`
- `vllm_blend/worker/model_runner.py`
- `vllm_blend/platform.py`
- `vllm_blend/__init__.py`
- `setup.py`

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° BlendPlatform
- [ ] å®ç° BlendWorker
- [ ] å®ç° BlendModelRunnerMixin
- [ ] å®ç°æ’ä»¶æ³¨å†Œå‡½æ•°
- [ ] åˆ›å»º setup.py
- [ ] ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•

### Phase 5: Ascend NPU æ”¯æŒ (Week 6)

**ç›®æ ‡**: å®ç° NPU backend

**å…³é”®æ–‡ä»¶**:
- `vllm_blend/backends/npu/gpu_provider.py`
- `vllm_blend/backends/npu/cache_provider.py`
- `vllm_blend/backends/npu/model_provider.py`

**ä»»åŠ¡æ¸…å•**:
- [ ] å®ç° NPUGPUProvider
- [ ] å®ç° NPUCacheProvider
- [ ] å®ç° NPUModelProvider
- [ ] ä¸ vLLM-Ascend KV ç®¡ç†é›†æˆ
- [ ] åœ¨ Ascend ç¡¬ä»¶ä¸Šæµ‹è¯•
- [ ] NPU ç‰¹å®šä¼˜åŒ–

### Phase 6: æµ‹è¯•ä¸æ–‡æ¡£ (Week 7-8)

**ç›®æ ‡**: å®Œå–„æµ‹è¯•å’Œæ–‡æ¡£

**ä»»åŠ¡æ¸…å•**:
- [ ] å®Œæ•´çš„å•å…ƒæµ‹è¯•è¦†ç›–
- [ ] é›†æˆæµ‹è¯•ï¼ˆå¤šæ¨¡å‹ï¼‰
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] ç”¨æˆ·æ–‡æ¡£
- [ ] API å‚è€ƒ
- [ ] ç¤ºä¾‹ä»£ç 
- [ ] README

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

```python
# tests/test_blender.py

def test_token_selector():
    """æµ‹è¯• Token é€‰æ‹©ç®—æ³•"""
    selector = TokenSelector(
        BlendCommonMetadata(check_layers=[0], recomp_ratios=[0.5])
    )

    new_k = torch.randn(100, 32, 128)
    old_k = torch.randn(100, 32, 128)

    indices = selector.select_important_tokens(new_k, old_k, ratio=0.5)

    assert len(indices) == 50
    assert torch.all(indices < 100)

def test_blender_with_mock_providers():
    """ä½¿ç”¨ mock providers æµ‹è¯• Blender"""
    blender = BlendBlender(
        cache_provider=MockCacheProvider(),
        gpu_provider=MockGPUProvider(),
        model_provider=MockModelProvider(),
        common_metadata=BlendCommonMetadata(
            check_layers=[0],
            recomp_ratios=[0.1]
        ),
    )

    q, k, v, residual = blender.process_qkv(
        q=torch.randn(10, 32, 128),
        k=torch.randn(10, 32, 128),
        v=torch.randn(10, 32, 128),
        residual=torch.randn(10, 4096),
        layer_id=0,
    )

    # åº”è¯¥åªè¿”å› 10% çš„ tokens
    assert q.shape[0] == 1
```

### 7.2 é›†æˆæµ‹è¯•

```python
# tests/integration/test_cuda.py

def test_blend_worker_cuda():
    """æµ‹è¯• CUDA ä¸Šçš„ Blend worker"""
    vllm_config = VllmConfig(
        model="meta-llama/Llama-2-7b-chat-hf",
        additional_config={
            "blend_config": {
                "enabled": True,
                "check_layers": [0, 16],
                "recompute_ratios": [0.1],
            }
        }
    )

    llm = LLM(config=vllm_config)
    outputs = llm.generate("Hello, world!", SamplingParams(max_tokens=10))

    assert len(outputs) == 1
    assert len(outputs[0].outputs[0].text) > 0
```

### 7.3 æ€§èƒ½æµ‹è¯•

```python
# benchmarks/benchmark_blend.py

def benchmark_blend_vs_baseline():
    """å¯¹æ¯” Blend å’Œ baseline çš„æ€§èƒ½"""

    prompts = ["Hello, world!"] * 100
    sampling_params = SamplingParams(max_tokens=100)

    # Baseline
    llm_baseline = LLM(model="meta-llama/Llama-2-7b-chat-hf")
    start = time.time()
    outputs_baseline = llm_baseline.generate(prompts, sampling_params)
    time_baseline = time.time() - start

    # With Blend
    llm_blend = LLM(
        model="meta-llama/Llama-2-7b-chat-hf",
        enable_blend=True,
        blend_recompute_ratios=[0.1],
    )
    start = time.time()
    outputs_blend = llm_blend.generate(prompts, sampling_params)
    time_blend = time.time() - start

    speedup = time_baseline / time_blend
    print(f"Blend åŠ é€Ÿæ¯”: {speedup:.2f}x")

    # éªŒè¯è¾“å‡ºè´¨é‡
    assert len(outputs_baseline) == len(outputs_blend)
```

## 8. å…³é”®è®¾è®¡å†³ç­–

### 8.1 Provider æŠ½è±¡æ¨¡å¼

**å†³ç­–**: ä½¿ç”¨ä¸‰ä¸ªæŠ½è±¡æ¥å£ï¼ˆCacheProvider, GPUProvider, ModelProviderï¼‰

**åŸå› **:
- å®Œå…¨è§£è€¦å…·ä½“å®ç°
- æ”¯æŒå¤šç§ç¡¬ä»¶å’Œç¼“å­˜åç«¯
- æ˜“äºæµ‹è¯•å’Œæ‰©å±•

**æƒè¡¡**:
- å¢åŠ äº†ä¸€å±‚æŠ½è±¡
- éœ€è¦ä¸ºæ¯ä¸ªå¹³å°å®ç° adapter

**ç»“è®º**: æ”¶ç›Šå¤§äºæˆæœ¬ï¼Œæ˜¯å®ç°ç¡¬ä»¶æ— å…³æ€§çš„æœ€ä½³æ–¹å¼

### 8.2 å¹³å°åŒ…è£… vs å¹³å°æ›¿æ¢

**å†³ç­–**: BlendPlatform åŒ…è£…åº•å±‚å¹³å°è€Œéæ›¿æ¢

**åŸå› **:
- ä¿ç•™åº•å±‚å¹³å°çš„æ‰€æœ‰åŠŸèƒ½
- Blend ä½œä¸ºå¯é€‰åŠŸèƒ½æ·»åŠ 
- ç”¨æˆ·å¯ä»¥åŒæ—¶ä½¿ç”¨å…¶ä»–å¹³å°ç‰¹æ€§

**å®ç°**:
```python
class BlendPlatform(Platform):
    @classmethod
    def get_attn_backend(cls, ...):
        # è·å–åº•å±‚å¹³å°çš„åç«¯
        base = get_underlying_platform()
        base_backend = base.get_attn_backend(...)

        # å¦‚æœå¯ç”¨ Blendï¼ŒåŒ…è£…å®ƒ
        if blend_enabled:
            return BlendBackendWrapper(base_backend)
        return base_backend
```

### 8.3 é€‚é…å™¨æ¨¡å¼

**å†³ç­–**: ä½¿ç”¨é€‚é…å™¨æ¨¡å¼æ”¯æŒä¸åŒæ¨¡å‹

**åŸå› **:
- ä¸åŒæ¨¡å‹çš„ QKV projection å®ç°ä¸åŒ
- é€‚é…å™¨å°è£…æ¨¡å‹ç‰¹å®šé€»è¾‘
- æ˜“äºæ·»åŠ æ–°æ¨¡å‹æ”¯æŒ

**å®ç°**:
```python
class LlamaAdapter(BaseModelAdapter):
    def extract_qkv(self, layer_id, hidden_states):
        # Llama ç‰¹å®šå®ç°
        pass

class Qwen3Adapter(BaseModelAdapter):
    def extract_qkv(self, layer_id, hidden_states):
        # Qwen3 ç‰¹å®šå®ç°
        pass
```

### 8.4 åˆ†å±‚æ‰§è¡Œ

**å†³ç­–**: ä¿æŒä¸ LMCache ç›¸åŒçš„ layer-wise æ‰§è¡Œæ–¹å¼

**åŸå› **:
- é€å±‚æ‰§è¡Œæ›´é€‚åˆæ··åˆé€»è¾‘
- å¯ä»¥åœ¨ç‰¹å®šå±‚ï¼ˆå¦‚ layer 0ï¼‰æ‰§è¡Œ token é€‰æ‹©
- å‡å°‘å†…å­˜å³°å€¼ä½¿ç”¨

**å®ç°**:
```python
def blend_layer(tokens, mask):
    for layer_id in range(num_layers):
        retrieve_layer(layer_id)
        compute_layer(layer_id)
        process_qkv(layer_id)  # æ··åˆ
        yield
```

## 9. å…¼å®¹æ€§ä¿è¯

### 9.1 API å…¼å®¹æ€§

- **å‘åå…¼å®¹**: ä¸ä¿®æ”¹ vLLM ç°æœ‰ API
- **å¯é€‰åŠŸèƒ½**: æœªå¯ç”¨æ—¶ä¸å½±å“æ€§èƒ½
- **æ¸è¿›é‡‡ç”¨**: ç”¨æˆ·å¯ä»¥é€‰æ‹©æ€§å¯ç”¨

### 9.2 æ¨¡å‹å…¼å®¹æ€§

æ”¯æŒçš„æ¨¡å‹æ¶æ„ï¼š
- âœ… Llama (Llama 1, 2, 3, Mistral, Mixtral)
- âœ… Qwen (Qwen 1.5, 2, 2.5, 3)
- âœ… å…¶ä»–åŸºäº Llama æ¶æ„çš„æ¨¡å‹

æ‰©å±•æ–°æ¨¡å‹éœ€è¦ï¼š
1. å®ç°é€‚é…å™¨ï¼ˆ~100 è¡Œä»£ç ï¼‰
2. æ³¨å†Œåˆ°é€‚é…å™¨è¡¨
3. æµ‹è¯•éªŒè¯

### 9.3 ç¡¬ä»¶å…¼å®¹æ€§

| ç¡¬ä»¶ | çŠ¶æ€ | å¤‡æ³¨ |
|------|------|------|
| CUDA (NVIDIA) | âœ… è®¡åˆ’æ”¯æŒ | Phase 2 |
| Ascend NPU | âœ… è®¡åˆ’æ”¯æŒ | Phase 5 |
| ROCm (AMD) | ğŸ”„ æœªæ¥æ”¯æŒ | Phase 7+ |
| Intel CPU/GPU | ğŸ”„ æœªæ¥æ”¯æŒ | å¾…å®š |

### 9.4 ä¸ LMCache å…±å­˜

- å¯ä»¥åŒæ—¶ä½¿ç”¨ LMCache å’Œ Blend
- Blend é€šè¿‡ LMCache cache provider è®¿é—® LMCache
- LMCache å¤„ç†å­˜å‚¨ï¼ŒBlend å¤„ç†æ··åˆé€»è¾‘

## 10. æ€§èƒ½é¢„æœŸ

### 10.1 ç†è®ºåˆ†æ

å‡è®¾åœºæ™¯ï¼šRAG åº”ç”¨ï¼Œ3ä¸ªæ–‡æ¡£ chunkï¼Œä¸åŒé¡ºåºç»„åˆ

| æŒ‡æ ‡ | æ—  Blend | æœ‰ Blend (15%) | æ”¹è¿› |
|------|---------|----------------|------|
| TTFT (é¦–æ¬¡) | 100ms | 100ms | 0% |
| TTFT (åç»­) | 100ms | ~30ms | 70% â†“ |
| GPU å†…å­˜ | 100% | 100% | 0% |
| Cache å¤ç”¨ | 0% | 85% | +85% |

### 10.2 å®é™…æµ‹é‡

æµ‹è¯•ç¯å¢ƒï¼š
- æ¨¡å‹: Llama-2-7b
- ç¡¬ä»¶: NVIDIA A100
- åœºæ™¯: å¤šæ–‡æ¡£ RAG

é¢„æœŸç»“æœï¼š
- TTFT å‡å°‘: 30-60%
- ååé‡æå‡: 1.5-2x
- è´¨é‡æŸå¤±: <1%ï¼ˆå›°æƒ‘åº¦å¯¹æ¯”ï¼‰

## 11. é£é™©ä¸æŒ‘æˆ˜

### 11.1 æŠ€æœ¯é£é™©

| é£é™© | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|----------|
| Provider æ¥å£è®¾è®¡ä¸å½“ | é«˜ | å……åˆ†çš„åŸå‹éªŒè¯ï¼Œè¿­ä»£è®¾è®¡ |
| æ€§èƒ½ä¸è¾¾é¢„æœŸ | ä¸­ | æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ– |
| ç¡¬ä»¶ç‰¹å®š bug | ä¸­ | æ—©æœŸæµ‹è¯•ï¼Œå•å…ƒæµ‹è¯•è¦†ç›– |
| ä¸ vLLM ç‰ˆæœ¬å…¼å®¹æ€§ | ä¸­ | ç‰ˆæœ¬é”å®šæµ‹è¯•ï¼ŒCI é›†æˆ |

### 11.2 å®æ–½æŒ‘æˆ˜

1. **è§£è€¦å¤æ‚åº¦**: LMCache Blend æ·±åº¦è€¦åˆï¼Œéœ€è¦ä»”ç»†æ¢³ç†ä¾èµ–
   - **è§£å†³æ–¹æ¡ˆ**: ä½¿ç”¨ mock providers æµ‹è¯•æ ¸å¿ƒé€»è¾‘ï¼Œé€æ­¥è¿ç§»

2. **ç¡¬ä»¶å·®å¼‚**: ä¸åŒç¡¬ä»¶çš„ KV ç®¡ç†æ–¹å¼ä¸åŒ
   - **è§£å†³æ–¹æ¡ˆ**: Provider æŠ½è±¡å±‚éš”ç¦»å·®å¼‚

3. **æµ‹è¯•è¦†ç›–**: éœ€è¦åœ¨å¤šç§ç¡¬ä»¶ä¸Šæµ‹è¯•
   - **è§£å†³æ–¹æ¡ˆ**: CI/CD é›†æˆï¼Œå®šæœŸæµ‹è¯•

## 12. åç»­ä¼˜åŒ–

### 12.1 Phase 7+ (æœªæ¥åŠŸèƒ½)

1. **æ›´å¤šç¡¬ä»¶æ”¯æŒ**
   - ROCm (AMD GPU)
   - Intel CPU/GPU
   - TPU

2. **é«˜çº§æ··åˆç­–ç•¥**
   - è‡ªé€‚åº”é‡ç®—æ¯”ä¾‹
   - åŸºäºé˜ˆå€¼çš„æ··åˆ
   - å¤šå±‚æ··åˆç­–ç•¥

3. **æ€§èƒ½ä¼˜åŒ–**
   - Kernel èåˆ
   - å¼‚æ­¥æ‰§è¡Œ
   - åˆ†å¸ƒå¼æ··åˆ

4. **åŠŸèƒ½æ‰©å±•**
   - æ”¯æŒæµå¼è¾“å…¥
   - æ”¯æŒå¤šæ¨¡æ€
   - æ”¯æŒæ‰¹å¤„ç†ä¼˜åŒ–

## 13. æ€»ç»“

æœ¬è®¾è®¡æ–¹æ¡ˆæä¾›äº†ä¸€ä¸ªå®Œå…¨è§£è€¦ã€ç¡¬ä»¶æ— å…³çš„ Blend æ’ä»¶ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š

1. **æ¸…æ™°æ¶æ„**: Provider æŠ½è±¡ + é€‚é…å™¨æ¨¡å¼
2. **æ˜“äºæ‰©å±•**: æ–°å¢ç¡¬ä»¶/æ¨¡å‹åªéœ€å®ç°æ¥å£
3. **æ€§èƒ½ä¼˜å…ˆ**: æœ€å°åŒ–å¼€é”€ï¼Œæœ€å¤§åŒ–å¤ç”¨
4. **ç”Ÿäº§å°±ç»ª**: å®Œæ•´çš„æµ‹è¯•å’Œæ–‡æ¡£è®¡åˆ’

é€šè¿‡åˆ†é˜¶æ®µå®æ–½ï¼Œå¯ä»¥é€æ­¥éªŒè¯å’Œä¼˜åŒ–ï¼Œç¡®ä¿é¡¹ç›®æˆåŠŸã€‚
